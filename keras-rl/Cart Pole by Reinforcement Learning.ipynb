{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipple Lecture #12 - Reinforcement Learning\n",
    "Now, you have seen quite some information relating to Reinforcement Learning. In this notebook, you will have the chance to program your own Deep Reinforcement Learning model. At least... tune its parameters. The programming of the game-environment, state-transitions, reward-calculations and training of the model has already been prepared for you. It is your job to focus on one task and one task only: keep your pole straight up!\n",
    "\n",
    "During the lecture, we have not been able to discuss all elements of a DRL-model, as there are many aspects which can be tuned to perfection (or far from it). Some additional explanation will be given in the notebook where deemed necessary, but don't be shy to ask more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Clone git-repo\n",
    "Clone necessary data and install missing packages. This may take a few minutes, but will only have to be ran once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Lecture_RL'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (0.8.27)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (0.14.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (5.4.8)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (7.352.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (3.13)\n",
      "Requirement already satisfied: watchdog>=0.8.3 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (0.10.2)\n",
      "Requirement already satisfied: gql==0.2.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (0.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (1.12.0)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (3.0.8)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (2.7.5)\n",
      "Requirement already satisfied: configparser>=3.8.1 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (4.0.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: Click>=7.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from wandb) (7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from sentry-sdk>=0.4.0->wandb) (2019.9.11)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from sentry-sdk>=0.4.0->wandb) (1.24.1)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
      "Requirement already satisfied: graphql-core<2,>=0.5.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from gql==0.2.0->wandb) (1.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from gql==0.2.0->wandb) (2.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: gitdb2>=3 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from GitPython>=1.0.0->wandb) (3.0.2)\n",
      "Requirement already satisfied: smmap2>=2.0.0 in c:\\users\\lennartvanham\\anaconda3\\lib\\site-packages (from gitdb2>=3->GitPython>=1.0.0->wandb) (2.0.5)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/PippleNL/Lecture_RL.git\n",
    "!pip install wandb\n",
    "!pip install tensorflow==1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set system path so the program understands where to find the relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "root_path = '/content/Lecture_RL/keras-rl'\n",
    "if root_path not in sys.path:\n",
    "  sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing relevant modules\n",
    "Let's get started. First, import necessary modules (and suppress some unwanted warnings). The 'gym' package is imported to be able to create a Cart Pole environment for you to play with. Further on, 'keras' enables the usage of a neural network, while 'keras-rl' contains a whole bunch of interesting Reinforcement Learning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting variables\n",
    "Then, set the relevant variables. Get the environment and extract the number of actions available in the Cartpole problem. The seed settings can be useful to compare your results over different runs. However, both a neural network as the RL framework itself still contain a high level of randomization, which may make comparison of distinct runs difficult. Keep this in mind when trying different parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up your neural network\n",
    "Next, build a neural network model. Initially, it is set to a simple feed-forward neural net, with a single hidden layer and 4 nodes. Try different settings by yourself, to find your optimal set-up! Unfortunately, until the day of today, there are no clear rules for choosing how many layers or nodes to use. Google may give you some idea, but most decisions still follow the famous method of trial-and-error.\n",
    "\n",
    "Try tuning the number of hidden layers, the number of nodes per hidden layer, and the type of activation functions in the hidden and output layers. Use the 'print(model.summary())' to get an overview of the complexity of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LennartvanHam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dense(4))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create your learning agent\n",
    "\n",
    "Now, configure and compile your agent. The memory is set to Sequential Memory, storing the result of performed actions and obtained rewards. Settings you can tune:\n",
    "\n",
    "* **policy**: the way in which actions are selected over time, following some balancing method. This RL-concept is very important, incorporating a trade-off between exploring unknown parts of the environment, and exploiting known information. (possible policies: EpsGreedyQPolicy, LinearAnnealedPolicy, SoftmaxPolicy, GreedyQPolicy, BoltzmannQPolicy, MaxBoltzmannQPolicy, BoltzmannGumbelQPolicy)\n",
    "* **memory limit**: the number of previous actions+rewards that are taken into account while learning, at a certain moment in time.\n",
    "* **window_length**: actually not sure... just keep it at 1 to avoid errors (or see it as a challenge to find out ;))\n",
    "* **target_model_update**: in RL-theory denoted by $\\alpha$, the network's learning rate. It determines how quickly the algorithm wants to converge to found target values (such as Q-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The long process of learning...\n",
    "Now it's time to learn something! There are four settings you can consider changing, however, only one which has an effect on your training performance:\n",
    "\n",
    "* **nb_steps**: the larger, the more time your bot gets for trying to find a good strategy, but the longer you'll have to wait.\n",
    "* **verbose**: printing running status. 0 for no logging, 1 for interval logging, 2 for episode logging\n",
    "* **log_interval**: if verbose=1, the number of steps that are considered to be an interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10/10 [==============================] - 0s 17ms/step - reward: 1.0000\n",
      "Interval 2 (10 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 3 (20 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 4 (30 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 5 (40 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 6 (50 steps performed)\n",
      "10/10 [==============================] - 0s 7ms/step - reward: 1.0000\n",
      "Interval 7 (60 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 8 (70 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 9 (80 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 10 (90 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 11 (100 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 12 (110 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 13 (120 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 14 (130 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 15 (140 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 16 (150 steps performed)\n",
      "10/10 [==============================] - ETA: 0s - reward: 1.000 - 0s 3ms/step - reward: 1.0000\n",
      "Interval 17 (160 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 18 (170 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 19 (180 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 20 (190 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 191.000 [191.000, 191.000]\n",
      "\n",
      "Interval 21 (200 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 22 (210 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 23 (220 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 30.000 [30.000, 30.000]\n",
      "\n",
      "Interval 24 (230 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 25 (240 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 26 (250 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 27 (260 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 28 (270 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 52.000 [52.000, 52.000]\n",
      "\n",
      "Interval 29 (280 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 30 (290 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 31 (300 steps performed)\n",
      "10/10 [==============================] - ETA: 0s - reward: 1.000 - 0s 2ms/step - reward: 1.0000\n",
      "Interval 32 (310 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 33 (320 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 34 (330 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 35 (340 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 36 (350 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 84.000 [84.000, 84.000]\n",
      "\n",
      "Interval 37 (360 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 38 (370 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 39 (380 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 40 (390 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 41 (400 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 52.000 [52.000, 52.000]\n",
      "\n",
      "Interval 42 (410 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 43 (420 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 44 (430 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 45 (440 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 46 (450 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 47 (460 steps performed)\n",
      "10/10 [==============================] - ETA: 0s - reward: 1.000 - 0s 3ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 58.000 [58.000, 58.000]\n",
      "\n",
      "Interval 48 (470 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 49 (480 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 50 (490 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 51 (500 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 52 (510 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 53 (520 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 63.000 [63.000, 63.000]\n",
      "\n",
      "Interval 54 (530 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 55 (540 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 56 (550 steps performed)\n",
      "10/10 [==============================] - 0s 1ms/step - reward: 1.0000\n",
      "Interval 57 (560 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 58 (570 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 59 (580 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 60 (590 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 61 (600 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 80.000 [80.000, 80.000]\n",
      "\n",
      "Interval 62 (610 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 63 (620 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 64 (630 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 65 (640 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 66 (650 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 46.000 [46.000, 46.000]\n",
      "\n",
      "Interval 67 (660 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 68 (670 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 69 (680 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 70 (690 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 71 (700 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 72 (710 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 73 (720 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 74 (730 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 75 (740 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 76 (750 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 77 (760 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 78 (770 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 79 (780 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 80 (790 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 81 (800 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 82 (810 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 83 (820 steps performed)\n",
      "10/10 [==============================] - 0s 3ms/step - reward: 1.0000\n",
      "Interval 84 (830 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 85 (840 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 86 (850 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 200.000 [200.000, 200.000]\n",
      "\n",
      "Interval 87 (860 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 88 (870 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 89 (880 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 90 (890 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 91 (900 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 92 (910 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 93 (920 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 94 (930 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 95 (940 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 96 (950 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 97 (960 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "1 episodes - episode_reward: 111.000 [111.000, 111.000]\n",
      "\n",
      "Interval 98 (970 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 99 (980 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "Interval 100 (990 steps performed)\n",
      "10/10 [==============================] - 0s 2ms/step - reward: 1.0000\n",
      "done, took 2.651 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16d720abcf8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000, verbose=1, log_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How well do you perform?\n",
    "Run the below code to test your DRL model. The larger the reward and number of steps per episode, the better your model performs. Running about 10 episodes will give you a proper overall status. Unfortunately, visualization only works when running locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 40.000, steps: 40\n",
      "Episode 2: reward: 46.000, steps: 46\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 49.000, steps: 49\n",
      "Episode 5: reward: 200.000, steps: 200\n",
      "Episode 6: reward: 82.000, steps: 82\n",
      "Episode 7: reward: 47.000, steps: 47\n",
      "Episode 8: reward: 55.000, steps: 55\n",
      "Episode 9: reward: 108.000, steps: 108\n",
      "Episode 10: reward: 94.000, steps: 94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16d008e06d8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Happy?\n",
    "If you are happy with your performance, save your model! Send it to lennart@pipple.nl, so it can be publicly evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "object_pkl = model\n",
    "file_pkl = open('model_[enter_team_name].obj', 'wb')\n",
    "pickle.dump(object_pkl, file_pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
