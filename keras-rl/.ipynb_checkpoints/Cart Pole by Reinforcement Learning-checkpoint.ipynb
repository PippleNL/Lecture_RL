{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipple Lecture #12 - Reinforcement Learning\n",
    "Now, you have seen quite some information relating to Reinforcement Learning. In this notebook, you will have the chance to program your own Deep Reinforcement Learning model. At least... tune its parameters. The programming of the game-environment, state-transitions, reward-calculations and training of the model has already been prepared for you. It is your job to focus on one task and one task only: keep your pole as straight as possible!\n",
    "\n",
    "During the lecture, we have not been able to discuss all elements of a DRL-model, as there are many aspects which can be tuned to perfection (or far from it). Some additional explanation will be given in the notebook where deemed necessary, but don't be shy to ask more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First, import necessary modules (and suppress some unwanted warnings). The 'gym' package is imported to be able to create a Cart Pole environment for you to play with. Further on, 'keras' enables the usage of a neural network, while 'keras-rl' contains a whole bunch of interesting Reinforcement Learning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, set the relevant variables. Get the environment and extract the number of actions available in the Cartpole problem. The seed settings can be useful to compare your results over different runs. However, both a neural network as the RL framework itself still contain a high level of randomization, which may make comparison of distinct runs difficult. Keep this in mind when trying different parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, build a neural network model. Initially, it is set to a simple feed-forward neural net, with a single hidden layer and 4 nodes (hint; this is probably quite low). Try different settings by yourself, to find your optimal set-up! Unfortunately, until the day of today, there are no clear rules for choosing how many layers or nodes to use. Google may give you some idea, but most decisions still follow the famous method of trial-and-error.\n",
    "\n",
    "Try tuning the number of hidden layers, the number of nodes per hidden layer, and the type of activation functions in the hidden and output layers. Use the 'print(model.summary())' to get an overview of the complexity of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dense(4))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, configure and compile your agent. The memory is set to Sequential Memory, storing the result of performed actions and obtained rewards. Try using different types of action-selection policies, memory sizes, learning rates, training steps, or w/e you can think of. Settings you can tune:\n",
    "\n",
    "* policy: the way in which actions are selected over time, following some balancing method. This RL-concept is very important, incorporating a trade-off between exploring unknown parts of the environment, and exploiting known information. (possible policies: EpsGreedyQPolicy, LinearAnnealedPolicy, SoftmaxPolicy, GreedyQPolicy, BoltzmannQPolicy, MaxBoltzmannQPolicy, BoltzmannGumbelQPolicy)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   97/5000: episode: 1, duration: 1.308s, episode steps: 97, steps per second: 74, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.118 [-0.757, 0.370], loss: 0.443661, mean_absolute_error: 0.495302, mean_q: 0.054613\n",
      "  151/5000: episode: 2, duration: 0.133s, episode steps: 54, steps per second: 406, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.132 [-0.237, 0.692], loss: 0.377329, mean_absolute_error: 0.513735, mean_q: 0.187438\n",
      "  217/5000: episode: 3, duration: 0.177s, episode steps: 66, steps per second: 374, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.153 [-0.210, 0.907], loss: 0.351174, mean_absolute_error: 0.549699, mean_q: 0.297660\n",
      "  288/5000: episode: 4, duration: 0.181s, episode steps: 71, steps per second: 392, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.194 [-0.306, 0.949], loss: 0.310265, mean_absolute_error: 0.594887, mean_q: 0.460893\n",
      "  355/5000: episode: 5, duration: 0.186s, episode steps: 67, steps per second: 361, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.035 [-0.630, 0.421], loss: 0.259104, mean_absolute_error: 0.663891, mean_q: 0.698324\n",
      "  384/5000: episode: 6, duration: 0.084s, episode steps: 29, steps per second: 344, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.110 [-0.641, 0.236], loss: 0.233247, mean_absolute_error: 0.734087, mean_q: 0.892726\n",
      "  411/5000: episode: 7, duration: 0.121s, episode steps: 27, steps per second: 223, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.113 [-0.680, 0.359], loss: 0.222795, mean_absolute_error: 0.783836, mean_q: 1.003322\n",
      "  436/5000: episode: 8, duration: 0.092s, episode steps: 25, steps per second: 270, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.092 [-0.714, 0.417], loss: 0.201584, mean_absolute_error: 0.822732, mean_q: 1.127860\n",
      "  465/5000: episode: 9, duration: 0.071s, episode steps: 29, steps per second: 411, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.078 [-0.695, 0.361], loss: 0.189315, mean_absolute_error: 0.873225, mean_q: 1.250975\n",
      "  494/5000: episode: 10, duration: 0.086s, episode steps: 29, steps per second: 339, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.067 [-0.803, 0.553], loss: 0.175381, mean_absolute_error: 0.933877, mean_q: 1.398548\n",
      "  515/5000: episode: 11, duration: 0.061s, episode steps: 21, steps per second: 347, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.068 [-0.867, 0.594], loss: 0.165544, mean_absolute_error: 0.988403, mean_q: 1.533257\n",
      "  531/5000: episode: 12, duration: 0.048s, episode steps: 16, steps per second: 332, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-0.866, 0.383], loss: 0.158523, mean_absolute_error: 1.037988, mean_q: 1.645678\n",
      "  553/5000: episode: 13, duration: 0.081s, episode steps: 22, steps per second: 273, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.049 [-0.907, 0.636], loss: 0.151660, mean_absolute_error: 1.093738, mean_q: 1.763835\n",
      "  566/5000: episode: 14, duration: 0.053s, episode steps: 13, steps per second: 245, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.124 [-1.284, 0.743], loss: 0.139917, mean_absolute_error: 1.135343, mean_q: 1.874107\n",
      "  582/5000: episode: 15, duration: 0.045s, episode steps: 16, steps per second: 359, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.082 [-1.131, 0.772], loss: 0.139765, mean_absolute_error: 1.174217, mean_q: 1.965471\n",
      "  595/5000: episode: 16, duration: 0.035s, episode steps: 13, steps per second: 369, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.108 [-1.354, 0.603], loss: 0.130776, mean_absolute_error: 1.217409, mean_q: 2.076513\n",
      "  609/5000: episode: 17, duration: 0.036s, episode steps: 14, steps per second: 384, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.108 [-1.245, 0.751], loss: 0.132142, mean_absolute_error: 1.260581, mean_q: 2.165813\n",
      "  625/5000: episode: 18, duration: 0.044s, episode steps: 16, steps per second: 368, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.071 [-1.396, 0.971], loss: 0.146218, mean_absolute_error: 1.312569, mean_q: 2.261818\n",
      "  638/5000: episode: 19, duration: 0.042s, episode steps: 13, steps per second: 306, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.105 [-1.466, 0.931], loss: 0.138705, mean_absolute_error: 1.351519, mean_q: 2.367029\n",
      "  650/5000: episode: 20, duration: 0.042s, episode steps: 12, steps per second: 288, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.461, 0.985], loss: 0.127356, mean_absolute_error: 1.399320, mean_q: 2.490627\n",
      "  661/5000: episode: 21, duration: 0.056s, episode steps: 11, steps per second: 197, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.098 [-1.639, 1.009], loss: 0.142823, mean_absolute_error: 1.453975, mean_q: 2.567879\n",
      "  674/5000: episode: 22, duration: 0.058s, episode steps: 13, steps per second: 223, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.091 [-1.816, 1.169], loss: 0.185867, mean_absolute_error: 1.513194, mean_q: 2.677414\n",
      "  688/5000: episode: 23, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.070 [-1.926, 1.384], loss: 0.160573, mean_absolute_error: 1.552233, mean_q: 2.787729\n",
      "  697/5000: episode: 24, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.130 [-2.167, 1.381], loss: 0.205762, mean_absolute_error: 1.611592, mean_q: 2.884932\n",
      "  707/5000: episode: 25, duration: 0.031s, episode steps: 10, steps per second: 327, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.102 [-2.313, 1.608], loss: 0.181920, mean_absolute_error: 1.622154, mean_q: 2.923827\n",
      "  716/5000: episode: 26, duration: 0.027s, episode steps: 9, steps per second: 329, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.153 [-2.168, 1.359], loss: 0.209698, mean_absolute_error: 1.663953, mean_q: 3.012364\n",
      "  725/5000: episode: 27, duration: 0.027s, episode steps: 9, steps per second: 331, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.160 [-2.479, 1.532], loss: 0.232248, mean_absolute_error: 1.683562, mean_q: 3.046016\n",
      "  737/5000: episode: 28, duration: 0.037s, episode steps: 12, steps per second: 326, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.121 [-3.102, 1.955], loss: 0.270472, mean_absolute_error: 1.742751, mean_q: 3.122394\n",
      "  746/5000: episode: 29, duration: 0.041s, episode steps: 9, steps per second: 221, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.753, 1.770], loss: 0.219124, mean_absolute_error: 1.776911, mean_q: 3.237060\n",
      "  756/5000: episode: 30, duration: 0.032s, episode steps: 10, steps per second: 310, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.126, 1.950], loss: 0.228879, mean_absolute_error: 1.830844, mean_q: 3.338762\n",
      "  766/5000: episode: 31, duration: 0.027s, episode steps: 10, steps per second: 370, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.119 [-3.035, 1.981], loss: 0.284028, mean_absolute_error: 1.884977, mean_q: 3.468330\n",
      "  774/5000: episode: 32, duration: 0.021s, episode steps: 8, steps per second: 373, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.543, 1.586], loss: 0.394551, mean_absolute_error: 1.954227, mean_q: 3.533156\n",
      "  787/5000: episode: 33, duration: 0.033s, episode steps: 13, steps per second: 393, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.107 [-2.774, 1.779], loss: 0.238536, mean_absolute_error: 1.923260, mean_q: 3.555492\n",
      "  798/5000: episode: 34, duration: 0.030s, episode steps: 11, steps per second: 362, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.323, 2.169], loss: 0.355142, mean_absolute_error: 1.984331, mean_q: 3.649570\n",
      "  808/5000: episode: 35, duration: 0.027s, episode steps: 10, steps per second: 367, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.152 [-2.509, 1.522], loss: 0.403003, mean_absolute_error: 2.092624, mean_q: 3.810406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  818/5000: episode: 36, duration: 0.031s, episode steps: 10, steps per second: 323, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-3.033, 1.914], loss: 0.280539, mean_absolute_error: 2.027750, mean_q: 3.787840\n",
      "  827/5000: episode: 37, duration: 0.050s, episode steps: 9, steps per second: 181, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.171 [-2.884, 1.742], loss: 0.445651, mean_absolute_error: 2.134821, mean_q: 3.904449\n",
      "  838/5000: episode: 38, duration: 0.045s, episode steps: 11, steps per second: 247, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.117 [-3.298, 2.182], loss: 0.407343, mean_absolute_error: 2.168484, mean_q: 4.011133\n",
      "  849/5000: episode: 39, duration: 0.040s, episode steps: 11, steps per second: 278, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.134 [-2.815, 1.741], loss: 0.414828, mean_absolute_error: 2.209530, mean_q: 4.058850\n",
      "  858/5000: episode: 40, duration: 0.038s, episode steps: 9, steps per second: 240, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.821, 1.779], loss: 0.422616, mean_absolute_error: 2.242410, mean_q: 4.102767\n",
      "  869/5000: episode: 41, duration: 0.051s, episode steps: 11, steps per second: 216, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.131 [-2.267, 1.352], loss: 0.540909, mean_absolute_error: 2.317498, mean_q: 4.205704\n",
      "  879/5000: episode: 42, duration: 0.043s, episode steps: 10, steps per second: 235, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.676, 1.746], loss: 0.492027, mean_absolute_error: 2.299622, mean_q: 4.183861\n",
      "  889/5000: episode: 43, duration: 0.037s, episode steps: 10, steps per second: 270, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.145 [-2.485, 1.537], loss: 0.481934, mean_absolute_error: 2.341417, mean_q: 4.249478\n",
      "  898/5000: episode: 44, duration: 0.038s, episode steps: 9, steps per second: 237, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.481, 1.521], loss: 0.434003, mean_absolute_error: 2.338240, mean_q: 4.312841\n",
      "  906/5000: episode: 45, duration: 0.033s, episode steps: 8, steps per second: 240, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.491, 1.582], loss: 0.443772, mean_absolute_error: 2.401007, mean_q: 4.437742\n",
      "  916/5000: episode: 46, duration: 0.041s, episode steps: 10, steps per second: 247, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.136 [-2.035, 1.195], loss: 0.548095, mean_absolute_error: 2.457133, mean_q: 4.504283\n",
      "  926/5000: episode: 47, duration: 0.041s, episode steps: 10, steps per second: 243, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.465, 1.551], loss: 0.602732, mean_absolute_error: 2.510354, mean_q: 4.599649\n",
      "  936/5000: episode: 48, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.449, 1.540], loss: 0.575449, mean_absolute_error: 2.514024, mean_q: 4.627645\n",
      "  946/5000: episode: 49, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.124 [-2.358, 1.549], loss: 0.759170, mean_absolute_error: 2.618519, mean_q: 4.695746\n",
      "  957/5000: episode: 50, duration: 0.045s, episode steps: 11, steps per second: 245, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.119 [-2.270, 1.386], loss: 0.619770, mean_absolute_error: 2.604970, mean_q: 4.704494\n",
      "  967/5000: episode: 51, duration: 0.038s, episode steps: 10, steps per second: 262, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-2.399, 1.566], loss: 0.662141, mean_absolute_error: 2.616957, mean_q: 4.755096\n",
      "  978/5000: episode: 52, duration: 0.044s, episode steps: 11, steps per second: 252, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.391, 1.554], loss: 0.592854, mean_absolute_error: 2.666275, mean_q: 4.869952\n",
      "  987/5000: episode: 53, duration: 0.034s, episode steps: 9, steps per second: 263, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.471, 1.527], loss: 0.645823, mean_absolute_error: 2.690410, mean_q: 4.936423\n",
      "  996/5000: episode: 54, duration: 0.032s, episode steps: 9, steps per second: 283, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.131 [-2.451, 1.605], loss: 0.610685, mean_absolute_error: 2.683955, mean_q: 4.965677\n",
      " 1006/5000: episode: 55, duration: 0.039s, episode steps: 10, steps per second: 259, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.659, 1.713], loss: 0.532898, mean_absolute_error: 2.731725, mean_q: 5.079453\n",
      " 1016/5000: episode: 56, duration: 0.039s, episode steps: 10, steps per second: 255, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.115 [-2.474, 1.582], loss: 0.436906, mean_absolute_error: 2.705153, mean_q: 5.114861\n",
      " 1026/5000: episode: 57, duration: 0.042s, episode steps: 10, steps per second: 236, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.139 [-2.702, 1.780], loss: 0.536306, mean_absolute_error: 2.787398, mean_q: 5.232199\n",
      " 1037/5000: episode: 58, duration: 0.049s, episode steps: 11, steps per second: 223, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.104 [-2.444, 1.604], loss: 0.711594, mean_absolute_error: 2.852035, mean_q: 5.303555\n",
      " 1046/5000: episode: 59, duration: 0.038s, episode steps: 9, steps per second: 237, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.148 [-2.467, 1.543], loss: 0.590935, mean_absolute_error: 2.876173, mean_q: 5.359395\n",
      " 1056/5000: episode: 60, duration: 0.045s, episode steps: 10, steps per second: 223, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.718, 1.780], loss: 0.726183, mean_absolute_error: 2.931767, mean_q: 5.388136\n",
      " 1064/5000: episode: 61, duration: 0.036s, episode steps: 8, steps per second: 224, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.534, 1.591], loss: 0.770660, mean_absolute_error: 2.962994, mean_q: 5.429936\n",
      " 1074/5000: episode: 62, duration: 0.036s, episode steps: 10, steps per second: 275, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.114 [-2.704, 1.800], loss: 0.462650, mean_absolute_error: 2.919173, mean_q: 5.481517\n",
      " 1084/5000: episode: 63, duration: 0.033s, episode steps: 10, steps per second: 301, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.156 [-2.599, 1.550], loss: 0.547733, mean_absolute_error: 2.962061, mean_q: 5.598310\n",
      " 1094/5000: episode: 64, duration: 0.043s, episode steps: 10, steps per second: 234, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.080, 1.980], loss: 0.884477, mean_absolute_error: 3.017603, mean_q: 5.640044\n",
      " 1102/5000: episode: 65, duration: 0.035s, episode steps: 8, steps per second: 230, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.546, 1.582], loss: 0.517622, mean_absolute_error: 2.966475, mean_q: 5.654039\n",
      " 1110/5000: episode: 66, duration: 0.029s, episode steps: 8, steps per second: 272, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.604, 1.560], loss: 0.717237, mean_absolute_error: 3.044419, mean_q: 5.731157\n",
      " 1120/5000: episode: 67, duration: 0.040s, episode steps: 10, steps per second: 250, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.134 [-2.467, 1.521], loss: 0.568804, mean_absolute_error: 3.018987, mean_q: 5.768317\n",
      " 1128/5000: episode: 68, duration: 0.030s, episode steps: 8, steps per second: 264, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.562, 1.559], loss: 0.663866, mean_absolute_error: 3.110736, mean_q: 5.850319\n",
      " 1136/5000: episode: 69, duration: 0.031s, episode steps: 8, steps per second: 262, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.564, 1.599], loss: 0.816864, mean_absolute_error: 3.123150, mean_q: 5.916695\n",
      " 1146/5000: episode: 70, duration: 0.036s, episode steps: 10, steps per second: 276, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.060, 1.948], loss: 0.828379, mean_absolute_error: 3.142730, mean_q: 5.947114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1154/5000: episode: 71, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.580, 1.613], loss: 0.699010, mean_absolute_error: 3.112987, mean_q: 5.978102\n",
      " 1165/5000: episode: 72, duration: 0.036s, episode steps: 11, steps per second: 307, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.746, 1.752], loss: 0.775495, mean_absolute_error: 3.199697, mean_q: 6.012989\n",
      " 1173/5000: episode: 73, duration: 0.021s, episode steps: 8, steps per second: 388, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.579, 1.552], loss: 0.591940, mean_absolute_error: 3.205439, mean_q: 6.047392\n",
      " 1183/5000: episode: 74, duration: 0.027s, episode steps: 10, steps per second: 372, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.075, 1.944], loss: 0.741739, mean_absolute_error: 3.220362, mean_q: 6.145784\n",
      " 1193/5000: episode: 75, duration: 0.025s, episode steps: 10, steps per second: 395, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.145 [-2.585, 1.536], loss: 0.730668, mean_absolute_error: 3.238420, mean_q: 6.196216\n",
      " 1202/5000: episode: 76, duration: 0.027s, episode steps: 9, steps per second: 330, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.877, 1.803], loss: 0.582132, mean_absolute_error: 3.230498, mean_q: 6.206721\n",
      " 1212/5000: episode: 77, duration: 0.027s, episode steps: 10, steps per second: 366, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-3.093, 1.964], loss: 0.744928, mean_absolute_error: 3.280593, mean_q: 6.228935\n",
      " 1223/5000: episode: 78, duration: 0.031s, episode steps: 11, steps per second: 360, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.112 [-2.786, 1.767], loss: 0.855935, mean_absolute_error: 3.359571, mean_q: 6.283275\n",
      " 1234/5000: episode: 79, duration: 0.039s, episode steps: 11, steps per second: 279, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.144 [-2.781, 1.729], loss: 0.797012, mean_absolute_error: 3.384826, mean_q: 6.294937\n",
      " 1242/5000: episode: 80, duration: 0.025s, episode steps: 8, steps per second: 326, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.126 [-1.975, 1.188], loss: 0.860550, mean_absolute_error: 3.386147, mean_q: 6.257618\n",
      " 1251/5000: episode: 81, duration: 0.024s, episode steps: 9, steps per second: 369, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.812, 1.773], loss: 0.707124, mean_absolute_error: 3.397952, mean_q: 6.368928\n",
      " 1264/5000: episode: 82, duration: 0.034s, episode steps: 13, steps per second: 378, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.109 [-2.764, 1.749], loss: 0.655168, mean_absolute_error: 3.393823, mean_q: 6.412356\n",
      " 1273/5000: episode: 83, duration: 0.025s, episode steps: 9, steps per second: 364, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.791, 1.763], loss: 0.686128, mean_absolute_error: 3.443580, mean_q: 6.527434\n",
      " 1283/5000: episode: 84, duration: 0.028s, episode steps: 10, steps per second: 359, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.133 [-2.487, 1.585], loss: 0.783967, mean_absolute_error: 3.489234, mean_q: 6.617391\n",
      " 1294/5000: episode: 85, duration: 0.031s, episode steps: 11, steps per second: 353, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.608, 1.784], loss: 0.786422, mean_absolute_error: 3.495538, mean_q: 6.490602\n",
      " 1303/5000: episode: 86, duration: 0.029s, episode steps: 9, steps per second: 306, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.447, 1.524], loss: 0.675605, mean_absolute_error: 3.496252, mean_q: 6.493586\n",
      " 1312/5000: episode: 87, duration: 0.033s, episode steps: 9, steps per second: 275, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.142 [-2.461, 1.534], loss: 0.637451, mean_absolute_error: 3.531970, mean_q: 6.603509\n",
      " 1322/5000: episode: 88, duration: 0.032s, episode steps: 10, steps per second: 310, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.149 [-2.778, 1.781], loss: 0.790109, mean_absolute_error: 3.531076, mean_q: 6.586870\n",
      " 1331/5000: episode: 89, duration: 0.028s, episode steps: 9, steps per second: 322, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.147 [-2.501, 1.591], loss: 0.609362, mean_absolute_error: 3.489924, mean_q: 6.570768\n",
      " 1341/5000: episode: 90, duration: 0.028s, episode steps: 10, steps per second: 355, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.621, 1.710], loss: 0.880875, mean_absolute_error: 3.591574, mean_q: 6.744018\n",
      " 1352/5000: episode: 91, duration: 0.030s, episode steps: 11, steps per second: 363, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.130 [-2.421, 1.520], loss: 0.669551, mean_absolute_error: 3.606691, mean_q: 6.793877\n",
      " 1361/5000: episode: 92, duration: 0.024s, episode steps: 9, steps per second: 372, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.164 [-2.499, 1.532], loss: 0.601433, mean_absolute_error: 3.628296, mean_q: 6.903697\n",
      " 1369/5000: episode: 93, duration: 0.023s, episode steps: 8, steps per second: 351, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.552, 1.581], loss: 0.697489, mean_absolute_error: 3.639156, mean_q: 6.909994\n",
      " 1379/5000: episode: 94, duration: 0.034s, episode steps: 10, steps per second: 291, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-3.095, 1.911], loss: 0.655823, mean_absolute_error: 3.643205, mean_q: 6.933801\n",
      " 1390/5000: episode: 95, duration: 0.031s, episode steps: 11, steps per second: 350, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.127 [-2.793, 1.801], loss: 0.655665, mean_absolute_error: 3.627338, mean_q: 6.902189\n",
      " 1402/5000: episode: 96, duration: 0.034s, episode steps: 12, steps per second: 358, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.099 [-2.703, 1.793], loss: 0.634662, mean_absolute_error: 3.655750, mean_q: 6.965318\n",
      " 1411/5000: episode: 97, duration: 0.026s, episode steps: 9, steps per second: 348, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.760, 1.759], loss: 0.614962, mean_absolute_error: 3.640919, mean_q: 6.913994\n",
      " 1420/5000: episode: 98, duration: 0.025s, episode steps: 9, steps per second: 354, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.463, 1.519], loss: 0.901466, mean_absolute_error: 3.728270, mean_q: 6.991027\n",
      " 1428/5000: episode: 99, duration: 0.023s, episode steps: 8, steps per second: 344, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.151 [-2.224, 1.398], loss: 0.847535, mean_absolute_error: 3.728873, mean_q: 6.954107\n",
      " 1436/5000: episode: 100, duration: 0.024s, episode steps: 8, steps per second: 336, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.142 [-2.194, 1.365], loss: 0.642856, mean_absolute_error: 3.656901, mean_q: 6.842216\n",
      " 1444/5000: episode: 101, duration: 0.022s, episode steps: 8, steps per second: 358, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.137 [-2.180, 1.402], loss: 0.753903, mean_absolute_error: 3.767434, mean_q: 7.079769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1454/5000: episode: 102, duration: 0.034s, episode steps: 10, steps per second: 298, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.118 [-2.392, 1.557], loss: 0.716644, mean_absolute_error: 3.755096, mean_q: 7.029856\n",
      " 1467/5000: episode: 103, duration: 0.036s, episode steps: 13, steps per second: 358, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.106 [-1.901, 1.158], loss: 0.646353, mean_absolute_error: 3.723486, mean_q: 6.975456\n",
      " 1478/5000: episode: 104, duration: 0.029s, episode steps: 11, steps per second: 376, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.108 [-2.367, 1.600], loss: 0.557665, mean_absolute_error: 3.719927, mean_q: 7.020200\n",
      " 1491/5000: episode: 105, duration: 0.036s, episode steps: 13, steps per second: 363, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.082 [-2.533, 1.776], loss: 0.542217, mean_absolute_error: 3.794462, mean_q: 7.255656\n",
      " 1500/5000: episode: 106, duration: 0.024s, episode steps: 9, steps per second: 374, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.150 [-2.172, 1.346], loss: 0.631035, mean_absolute_error: 3.858215, mean_q: 7.341365\n",
      " 1511/5000: episode: 107, duration: 0.032s, episode steps: 11, steps per second: 341, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.118 [-2.056, 1.385], loss: 0.537474, mean_absolute_error: 3.878008, mean_q: 7.369687\n",
      " 1521/5000: episode: 108, duration: 0.027s, episode steps: 10, steps per second: 370, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.144 [-1.816, 1.135], loss: 0.600836, mean_absolute_error: 3.812821, mean_q: 7.155339\n",
      " 1533/5000: episode: 109, duration: 0.039s, episode steps: 12, steps per second: 308, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.079 [-1.976, 1.402], loss: 0.652345, mean_absolute_error: 3.846299, mean_q: 7.219816\n",
      " 1545/5000: episode: 110, duration: 0.033s, episode steps: 12, steps per second: 361, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.101 [-1.768, 1.174], loss: 0.676311, mean_absolute_error: 3.888399, mean_q: 7.276478\n",
      " 1557/5000: episode: 111, duration: 0.032s, episode steps: 12, steps per second: 371, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.088 [-1.530, 1.020], loss: 0.676829, mean_absolute_error: 3.847049, mean_q: 7.177258\n",
      " 1567/5000: episode: 112, duration: 0.028s, episode steps: 10, steps per second: 355, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.117 [-1.822, 1.156], loss: 0.604596, mean_absolute_error: 3.870589, mean_q: 7.334107\n",
      " 1577/5000: episode: 113, duration: 0.032s, episode steps: 10, steps per second: 312, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.124 [-1.844, 1.172], loss: 0.687391, mean_absolute_error: 4.006519, mean_q: 7.562275\n",
      " 1588/5000: episode: 114, duration: 0.033s, episode steps: 11, steps per second: 336, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.101 [-1.324, 0.836], loss: 0.487369, mean_absolute_error: 3.953375, mean_q: 7.496436\n",
      " 1601/5000: episode: 115, duration: 0.035s, episode steps: 13, steps per second: 367, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.085 [-1.709, 1.151], loss: 0.679021, mean_absolute_error: 3.989168, mean_q: 7.547904\n",
      " 1615/5000: episode: 116, duration: 0.047s, episode steps: 14, steps per second: 296, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-1.772, 1.126], loss: 0.656935, mean_absolute_error: 3.955320, mean_q: 7.407650\n",
      " 1626/5000: episode: 117, duration: 0.030s, episode steps: 11, steps per second: 366, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.112 [-1.334, 0.777], loss: 0.757796, mean_absolute_error: 4.084239, mean_q: 7.618068\n",
      " 1643/5000: episode: 118, duration: 0.045s, episode steps: 17, steps per second: 380, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.060 [-1.393, 1.016], loss: 0.644895, mean_absolute_error: 3.986633, mean_q: 7.409193\n",
      " 1656/5000: episode: 119, duration: 0.037s, episode steps: 13, steps per second: 356, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.090 [-1.229, 0.807], loss: 0.701011, mean_absolute_error: 4.068207, mean_q: 7.626300\n",
      " 1672/5000: episode: 120, duration: 0.043s, episode steps: 16, steps per second: 371, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.109 [-1.497, 0.989], loss: 0.801546, mean_absolute_error: 4.026033, mean_q: 7.492664\n",
      " 1687/5000: episode: 121, duration: 0.041s, episode steps: 15, steps per second: 367, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.110 [-1.279, 0.743], loss: 0.854978, mean_absolute_error: 4.138564, mean_q: 7.698723\n",
      " 1705/5000: episode: 122, duration: 0.053s, episode steps: 18, steps per second: 337, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-0.941, 0.603], loss: 0.752121, mean_absolute_error: 4.102167, mean_q: 7.638977\n",
      " 1724/5000: episode: 123, duration: 0.062s, episode steps: 19, steps per second: 305, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.107 [-0.930, 0.573], loss: 0.568255, mean_absolute_error: 4.169547, mean_q: 7.788062\n",
      " 1747/5000: episode: 124, duration: 0.068s, episode steps: 23, steps per second: 340, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.102 [-0.902, 0.573], loss: 0.612451, mean_absolute_error: 4.145553, mean_q: 7.749370\n",
      " 1861/5000: episode: 125, duration: 0.292s, episode steps: 114, steps per second: 390, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.223 [-1.768, 0.633], loss: 0.686117, mean_absolute_error: 4.303537, mean_q: 8.073207\n",
      " 1892/5000: episode: 126, duration: 0.080s, episode steps: 31, steps per second: 386, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.128 [-0.199, 0.842], loss: 0.763816, mean_absolute_error: 4.401327, mean_q: 8.260201\n",
      " 1913/5000: episode: 127, duration: 0.053s, episode steps: 21, steps per second: 393, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.103 [-0.368, 0.964], loss: 0.696089, mean_absolute_error: 4.455189, mean_q: 8.423623\n",
      " 1928/5000: episode: 128, duration: 0.040s, episode steps: 15, steps per second: 373, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.094 [-0.774, 1.365], loss: 0.784907, mean_absolute_error: 4.587759, mean_q: 8.694464\n",
      " 1941/5000: episode: 129, duration: 0.035s, episode steps: 13, steps per second: 372, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.096 [-0.631, 1.160], loss: 0.823892, mean_absolute_error: 4.497492, mean_q: 8.469674\n",
      " 1954/5000: episode: 130, duration: 0.043s, episode steps: 13, steps per second: 302, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.095 [-0.805, 1.253], loss: 1.091518, mean_absolute_error: 4.569277, mean_q: 8.561065\n",
      " 1966/5000: episode: 131, duration: 0.033s, episode steps: 12, steps per second: 363, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.106 [-0.805, 1.407], loss: 0.528326, mean_absolute_error: 4.673110, mean_q: 8.901140\n",
      " 1979/5000: episode: 132, duration: 0.038s, episode steps: 13, steps per second: 344, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.123 [-0.778, 1.437], loss: 0.938307, mean_absolute_error: 4.724835, mean_q: 8.940586\n",
      " 1991/5000: episode: 133, duration: 0.033s, episode steps: 12, steps per second: 367, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.090 [-0.833, 1.277], loss: 1.230899, mean_absolute_error: 4.741118, mean_q: 8.884686\n",
      " 2001/5000: episode: 134, duration: 0.027s, episode steps: 10, steps per second: 370, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.133 [-0.784, 1.368], loss: 0.858325, mean_absolute_error: 4.700348, mean_q: 8.883825\n",
      " 2013/5000: episode: 135, duration: 0.032s, episode steps: 12, steps per second: 374, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.115 [-1.000, 1.658], loss: 1.668199, mean_absolute_error: 4.858149, mean_q: 9.041530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2022/5000: episode: 136, duration: 0.038s, episode steps: 9, steps per second: 238, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.155 [-1.147, 1.927], loss: 0.986024, mean_absolute_error: 4.802388, mean_q: 9.050751\n",
      " 2034/5000: episode: 137, duration: 0.034s, episode steps: 12, steps per second: 352, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.100 [-1.383, 2.147], loss: 1.059962, mean_absolute_error: 4.775156, mean_q: 8.999541\n",
      " 2045/5000: episode: 138, duration: 0.030s, episode steps: 11, steps per second: 369, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.109 [-1.415, 2.192], loss: 0.912245, mean_absolute_error: 4.867273, mean_q: 9.215308\n",
      " 2054/5000: episode: 139, duration: 0.034s, episode steps: 9, steps per second: 264, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.148 [-1.367, 2.286], loss: 0.785556, mean_absolute_error: 4.852686, mean_q: 9.268541\n",
      " 2064/5000: episode: 140, duration: 0.030s, episode steps: 10, steps per second: 338, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.330, 2.249], loss: 0.764809, mean_absolute_error: 4.891865, mean_q: 9.334155\n",
      " 2074/5000: episode: 141, duration: 0.028s, episode steps: 10, steps per second: 353, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.126 [-1.412, 2.218], loss: 1.050895, mean_absolute_error: 4.914445, mean_q: 9.295770\n",
      " 2085/5000: episode: 142, duration: 0.030s, episode steps: 11, steps per second: 362, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.114 [-1.399, 2.215], loss: 1.552971, mean_absolute_error: 4.998832, mean_q: 9.405796\n",
      " 2095/5000: episode: 143, duration: 0.028s, episode steps: 10, steps per second: 356, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.383, 2.265], loss: 1.540600, mean_absolute_error: 5.097562, mean_q: 9.589887\n",
      " 2104/5000: episode: 144, duration: 0.030s, episode steps: 9, steps per second: 303, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.151 [-1.373, 2.262], loss: 1.205778, mean_absolute_error: 5.086111, mean_q: 9.674798\n",
      " 2114/5000: episode: 145, duration: 0.032s, episode steps: 10, steps per second: 308, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.108 [-1.611, 2.504], loss: 1.644571, mean_absolute_error: 5.226319, mean_q: 9.824388\n",
      " 2125/5000: episode: 146, duration: 0.036s, episode steps: 11, steps per second: 306, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.134 [-1.732, 2.704], loss: 1.939515, mean_absolute_error: 5.278772, mean_q: 9.875826\n",
      " 2134/5000: episode: 147, duration: 0.030s, episode steps: 9, steps per second: 300, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.141 [-1.713, 2.777], loss: 1.204854, mean_absolute_error: 5.141728, mean_q: 9.756533\n",
      " 2144/5000: episode: 148, duration: 0.032s, episode steps: 10, steps per second: 317, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.915, 3.017], loss: 2.200835, mean_absolute_error: 5.344001, mean_q: 9.946907\n",
      " 2153/5000: episode: 149, duration: 0.027s, episode steps: 9, steps per second: 331, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [-1.782, 2.799], loss: 2.174417, mean_absolute_error: 5.307726, mean_q: 9.813046\n",
      " 2163/5000: episode: 150, duration: 0.028s, episode steps: 10, steps per second: 361, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.148 [-1.532, 2.507], loss: 1.265429, mean_absolute_error: 5.170514, mean_q: 9.736479\n",
      " 2174/5000: episode: 151, duration: 0.037s, episode steps: 11, steps per second: 301, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-2.133, 3.296], loss: 1.221542, mean_absolute_error: 5.369625, mean_q: 10.055750\n",
      " 2184/5000: episode: 152, duration: 0.030s, episode steps: 10, steps per second: 329, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.988, 3.051], loss: 2.733435, mean_absolute_error: 5.450939, mean_q: 10.077486\n",
      " 2192/5000: episode: 153, duration: 0.023s, episode steps: 8, steps per second: 344, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [-1.522, 2.524], loss: 1.224974, mean_absolute_error: 5.356705, mean_q: 10.135704\n",
      " 2201/5000: episode: 154, duration: 0.025s, episode steps: 9, steps per second: 356, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.808, 2.827], loss: 2.237767, mean_absolute_error: 5.504047, mean_q: 10.309832\n",
      " 2211/5000: episode: 155, duration: 0.028s, episode steps: 10, steps per second: 355, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.923, 3.008], loss: 1.418883, mean_absolute_error: 5.320567, mean_q: 10.007416\n",
      " 2220/5000: episode: 156, duration: 0.026s, episode steps: 9, steps per second: 348, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.742, 2.880], loss: 3.296525, mean_absolute_error: 5.540687, mean_q: 10.101561\n",
      " 2230/5000: episode: 157, duration: 0.030s, episode steps: 10, steps per second: 333, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.946, 2.965], loss: 1.698343, mean_absolute_error: 5.334945, mean_q: 9.903290\n",
      " 2244/5000: episode: 158, duration: 0.037s, episode steps: 14, steps per second: 377, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.091 [-1.585, 2.488], loss: 1.506452, mean_absolute_error: 5.428565, mean_q: 10.181295\n",
      " 2254/5000: episode: 159, duration: 0.034s, episode steps: 10, steps per second: 291, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.131 [-1.605, 2.496], loss: 1.611797, mean_absolute_error: 5.366496, mean_q: 10.105329\n",
      " 2264/5000: episode: 160, duration: 0.030s, episode steps: 10, steps per second: 338, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.099 [-1.779, 2.637], loss: 2.560128, mean_absolute_error: 5.551678, mean_q: 10.312660\n",
      " 2274/5000: episode: 161, duration: 0.027s, episode steps: 10, steps per second: 364, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.121 [-1.225, 2.028], loss: 1.887678, mean_absolute_error: 5.471862, mean_q: 10.238371\n",
      " 2286/5000: episode: 162, duration: 0.034s, episode steps: 12, steps per second: 354, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.095 [-1.589, 2.443], loss: 2.569248, mean_absolute_error: 5.473836, mean_q: 10.106157\n",
      " 2300/5000: episode: 163, duration: 0.038s, episode steps: 14, steps per second: 370, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.108 [-1.908, 2.989], loss: 2.671880, mean_absolute_error: 5.653287, mean_q: 10.419169\n",
      " 2311/5000: episode: 164, duration: 0.034s, episode steps: 11, steps per second: 328, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.136 [-1.785, 2.817], loss: 1.964544, mean_absolute_error: 5.511312, mean_q: 10.230149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2322/5000: episode: 165, duration: 0.034s, episode steps: 11, steps per second: 327, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.132 [-1.733, 2.768], loss: 1.826614, mean_absolute_error: 5.622189, mean_q: 10.496082\n",
      " 2330/5000: episode: 166, duration: 0.029s, episode steps: 8, steps per second: 280, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.131 [-1.390, 2.169], loss: 2.751743, mean_absolute_error: 5.686408, mean_q: 10.482021\n",
      " 2340/5000: episode: 167, duration: 0.027s, episode steps: 10, steps per second: 371, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.120 [-1.383, 2.173], loss: 3.520200, mean_absolute_error: 5.809319, mean_q: 10.588628\n",
      " 2349/5000: episode: 168, duration: 0.028s, episode steps: 9, steps per second: 323, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.163 [-1.363, 2.303], loss: 2.501343, mean_absolute_error: 5.679621, mean_q: 10.398562\n",
      " 2358/5000: episode: 169, duration: 0.024s, episode steps: 9, steps per second: 368, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.151 [-1.363, 2.321], loss: 2.634537, mean_absolute_error: 5.690350, mean_q: 10.449506\n",
      " 2370/5000: episode: 170, duration: 0.034s, episode steps: 12, steps per second: 354, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.141 [-1.336, 2.203], loss: 2.901567, mean_absolute_error: 5.788092, mean_q: 10.615502\n",
      " 2381/5000: episode: 171, duration: 0.030s, episode steps: 11, steps per second: 370, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-1.371, 2.033], loss: 2.359460, mean_absolute_error: 5.917516, mean_q: 10.922797\n",
      " 2392/5000: episode: 172, duration: 0.032s, episode steps: 11, steps per second: 340, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.121 [-1.134, 1.888], loss: 2.142472, mean_absolute_error: 5.789834, mean_q: 10.758976\n",
      " 2403/5000: episode: 173, duration: 0.034s, episode steps: 11, steps per second: 325, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.138 [-1.154, 1.960], loss: 2.359059, mean_absolute_error: 5.751393, mean_q: 10.668094\n",
      " 2411/5000: episode: 174, duration: 0.026s, episode steps: 8, steps per second: 311, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.135 [-1.396, 2.244], loss: 2.499366, mean_absolute_error: 5.841665, mean_q: 10.807579\n",
      " 2421/5000: episode: 175, duration: 0.034s, episode steps: 10, steps per second: 298, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.140 [-1.327, 2.203], loss: 2.795208, mean_absolute_error: 5.847823, mean_q: 10.820303\n",
      " 2432/5000: episode: 176, duration: 0.035s, episode steps: 11, steps per second: 312, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.103 [-1.365, 2.010], loss: 3.030758, mean_absolute_error: 5.959902, mean_q: 10.941387\n",
      " 2442/5000: episode: 177, duration: 0.031s, episode steps: 10, steps per second: 318, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.115 [-1.212, 1.977], loss: 1.758029, mean_absolute_error: 5.795581, mean_q: 10.818161\n",
      " 2451/5000: episode: 178, duration: 0.031s, episode steps: 9, steps per second: 293, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.132 [-1.145, 1.887], loss: 3.069787, mean_absolute_error: 5.823176, mean_q: 10.688312\n",
      " 2460/5000: episode: 179, duration: 0.034s, episode steps: 9, steps per second: 268, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.160 [-1.140, 1.909], loss: 2.670815, mean_absolute_error: 5.969694, mean_q: 10.968327\n",
      " 2471/5000: episode: 180, duration: 0.033s, episode steps: 11, steps per second: 330, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.131 [-0.945, 1.671], loss: 2.005412, mean_absolute_error: 5.845531, mean_q: 10.860958\n",
      " 2481/5000: episode: 181, duration: 0.028s, episode steps: 10, steps per second: 354, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.122 [-1.006, 1.587], loss: 2.831155, mean_absolute_error: 5.899164, mean_q: 10.780403\n",
      " 2495/5000: episode: 182, duration: 0.039s, episode steps: 14, steps per second: 356, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.087 [-1.001, 1.415], loss: 1.607141, mean_absolute_error: 5.792518, mean_q: 10.821184\n",
      " 2506/5000: episode: 183, duration: 0.032s, episode steps: 11, steps per second: 343, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.131 [-0.990, 1.811], loss: 2.682230, mean_absolute_error: 6.018876, mean_q: 11.119988\n",
      " 2517/5000: episode: 184, duration: 0.031s, episode steps: 11, steps per second: 358, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.118 [-0.782, 1.269], loss: 2.151295, mean_absolute_error: 5.772182, mean_q: 10.706994\n",
      " 2530/5000: episode: 185, duration: 0.041s, episode steps: 13, steps per second: 315, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.116 [-0.740, 1.387], loss: 2.665740, mean_absolute_error: 5.959528, mean_q: 10.984621\n",
      " 2543/5000: episode: 186, duration: 0.036s, episode steps: 13, steps per second: 361, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.084 [-0.620, 1.118], loss: 3.155615, mean_absolute_error: 6.015889, mean_q: 10.978873\n",
      " 2557/5000: episode: 187, duration: 0.037s, episode steps: 14, steps per second: 382, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.101 [-0.815, 1.459], loss: 2.569346, mean_absolute_error: 6.028883, mean_q: 11.074168\n",
      " 2567/5000: episode: 188, duration: 0.028s, episode steps: 10, steps per second: 359, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.140 [-0.962, 1.679], loss: 1.777982, mean_absolute_error: 5.907947, mean_q: 10.958471\n",
      " 2580/5000: episode: 189, duration: 0.035s, episode steps: 13, steps per second: 376, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.103 [-0.945, 1.687], loss: 2.016760, mean_absolute_error: 5.831855, mean_q: 10.834235\n",
      " 2593/5000: episode: 190, duration: 0.037s, episode steps: 13, steps per second: 347, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.093 [-1.013, 1.679], loss: 2.172407, mean_absolute_error: 6.022052, mean_q: 11.237091\n",
      " 2603/5000: episode: 191, duration: 0.028s, episode steps: 10, steps per second: 361, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.137 [-1.148, 1.815], loss: 2.879015, mean_absolute_error: 6.039351, mean_q: 11.196417\n",
      " 2618/5000: episode: 192, duration: 0.048s, episode steps: 15, steps per second: 314, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.087 [-1.150, 1.850], loss: 2.267842, mean_absolute_error: 6.168326, mean_q: 11.535435\n",
      " 2629/5000: episode: 193, duration: 0.031s, episode steps: 11, steps per second: 357, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.093 [-1.204, 1.823], loss: 2.026715, mean_absolute_error: 6.134958, mean_q: 11.533064\n",
      " 2640/5000: episode: 194, duration: 0.034s, episode steps: 11, steps per second: 328, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.117 [-1.194, 1.867], loss: 1.787103, mean_absolute_error: 5.945922, mean_q: 11.202908\n",
      " 2649/5000: episode: 195, duration: 0.025s, episode steps: 9, steps per second: 356, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.145 [-1.149, 1.957], loss: 2.279229, mean_absolute_error: 6.176518, mean_q: 11.569655\n",
      " 2661/5000: episode: 196, duration: 0.032s, episode steps: 12, steps per second: 374, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.111 [-1.365, 1.975], loss: 2.144768, mean_absolute_error: 6.055998, mean_q: 11.394605\n",
      " 2673/5000: episode: 197, duration: 0.033s, episode steps: 12, steps per second: 365, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.093 [-1.189, 1.964], loss: 4.174777, mean_absolute_error: 6.362779, mean_q: 11.595828\n",
      " 2684/5000: episode: 198, duration: 0.030s, episode steps: 11, steps per second: 366, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.122 [-1.171, 1.895], loss: 3.449782, mean_absolute_error: 6.353681, mean_q: 11.621799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2693/5000: episode: 199, duration: 0.034s, episode steps: 9, steps per second: 267, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.132 [-1.157, 1.940], loss: 3.408504, mean_absolute_error: 6.297012, mean_q: 11.509092\n",
      " 2702/5000: episode: 200, duration: 0.027s, episode steps: 9, steps per second: 337, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.135 [-1.423, 2.274], loss: 3.537980, mean_absolute_error: 6.351237, mean_q: 11.700573\n",
      " 2712/5000: episode: 201, duration: 0.027s, episode steps: 10, steps per second: 364, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.145 [-1.345, 2.169], loss: 2.966020, mean_absolute_error: 6.416731, mean_q: 11.887039\n",
      " 2722/5000: episode: 202, duration: 0.028s, episode steps: 10, steps per second: 360, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.111 [-1.416, 2.144], loss: 2.759087, mean_absolute_error: 6.203798, mean_q: 11.520238\n",
      " 2731/5000: episode: 203, duration: 0.026s, episode steps: 9, steps per second: 350, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.148 [-1.384, 2.166], loss: 2.702227, mean_absolute_error: 6.351303, mean_q: 11.791225\n",
      " 2744/5000: episode: 204, duration: 0.035s, episode steps: 13, steps per second: 368, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.121 [-1.136, 1.953], loss: 3.038623, mean_absolute_error: 6.327727, mean_q: 11.718843\n",
      " 2754/5000: episode: 205, duration: 0.027s, episode steps: 10, steps per second: 368, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.102 [-1.398, 2.058], loss: 3.319160, mean_absolute_error: 6.515921, mean_q: 12.007728\n",
      " 2764/5000: episode: 206, duration: 0.033s, episode steps: 10, steps per second: 299, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.146 [-1.132, 1.961], loss: 2.949863, mean_absolute_error: 6.342993, mean_q: 11.782879\n",
      " 2776/5000: episode: 207, duration: 0.037s, episode steps: 12, steps per second: 328, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.112 [-1.215, 1.997], loss: 3.116511, mean_absolute_error: 6.461071, mean_q: 11.872051\n",
      " 2788/5000: episode: 208, duration: 0.034s, episode steps: 12, steps per second: 350, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.121 [-1.166, 1.799], loss: 3.061496, mean_absolute_error: 6.350600, mean_q: 11.636950\n",
      " 2799/5000: episode: 209, duration: 0.030s, episode steps: 11, steps per second: 368, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-1.191, 1.897], loss: 2.758742, mean_absolute_error: 6.349329, mean_q: 11.754823\n",
      " 2809/5000: episode: 210, duration: 0.029s, episode steps: 10, steps per second: 350, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.104 [-1.202, 1.827], loss: 3.137824, mean_absolute_error: 6.270270, mean_q: 11.514259\n",
      " 2822/5000: episode: 211, duration: 0.034s, episode steps: 13, steps per second: 378, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.094 [-1.016, 1.696], loss: 3.158041, mean_absolute_error: 6.486489, mean_q: 11.915300\n",
      " 2832/5000: episode: 212, duration: 0.028s, episode steps: 10, steps per second: 361, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.100 [-1.004, 1.631], loss: 4.130823, mean_absolute_error: 6.521747, mean_q: 11.781065\n",
      " 2843/5000: episode: 213, duration: 0.037s, episode steps: 11, steps per second: 301, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.110 [-0.981, 1.545], loss: 4.505483, mean_absolute_error: 6.390204, mean_q: 11.448483\n",
      " 2858/5000: episode: 214, duration: 0.042s, episode steps: 15, steps per second: 354, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.102 [-0.767, 1.408], loss: 2.775521, mean_absolute_error: 6.390174, mean_q: 11.682466\n",
      " 2868/5000: episode: 215, duration: 0.028s, episode steps: 10, steps per second: 359, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.122 [-1.023, 1.571], loss: 3.446864, mean_absolute_error: 6.443303, mean_q: 11.707117\n",
      " 2879/5000: episode: 216, duration: 0.030s, episode steps: 11, steps per second: 367, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.103 [-0.835, 1.390], loss: 2.525939, mean_absolute_error: 6.402215, mean_q: 11.819668\n",
      " 2892/5000: episode: 217, duration: 0.035s, episode steps: 13, steps per second: 377, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.083 [-1.027, 1.553], loss: 3.415501, mean_absolute_error: 6.494564, mean_q: 11.827322\n",
      " 2902/5000: episode: 218, duration: 0.029s, episode steps: 10, steps per second: 341, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-0.749, 1.504], loss: 3.832296, mean_absolute_error: 6.448428, mean_q: 11.717530\n",
      " 2916/5000: episode: 219, duration: 0.037s, episode steps: 14, steps per second: 383, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.085 [-0.969, 1.450], loss: 2.185670, mean_absolute_error: 6.321963, mean_q: 11.722495\n",
      " 2926/5000: episode: 220, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.137 [-0.777, 1.423], loss: 3.107449, mean_absolute_error: 6.504504, mean_q: 11.946131\n",
      " 2938/5000: episode: 221, duration: 0.037s, episode steps: 12, steps per second: 327, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-0.755, 1.427], loss: 3.454290, mean_absolute_error: 6.565300, mean_q: 11.946849\n",
      " 2951/5000: episode: 222, duration: 0.041s, episode steps: 13, steps per second: 320, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.089 [-0.826, 1.311], loss: 3.277968, mean_absolute_error: 6.522524, mean_q: 11.930877\n",
      " 2963/5000: episode: 223, duration: 0.035s, episode steps: 12, steps per second: 345, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.116 [-0.742, 1.259], loss: 3.022751, mean_absolute_error: 6.293473, mean_q: 11.468131\n",
      " 2977/5000: episode: 224, duration: 0.043s, episode steps: 14, steps per second: 323, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.085 [-0.796, 1.257], loss: 3.740034, mean_absolute_error: 6.458575, mean_q: 11.699618\n",
      " 2994/5000: episode: 225, duration: 0.047s, episode steps: 17, steps per second: 358, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.084 [-0.735, 1.245], loss: 4.454844, mean_absolute_error: 6.587494, mean_q: 11.807589\n",
      " 3008/5000: episode: 226, duration: 0.047s, episode steps: 14, steps per second: 300, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.080 [-0.796, 1.163], loss: 2.497919, mean_absolute_error: 6.382632, mean_q: 11.724584\n",
      " 3027/5000: episode: 227, duration: 0.060s, episode steps: 19, steps per second: 317, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.090 [-0.611, 1.010], loss: 2.870946, mean_absolute_error: 6.423941, mean_q: 11.723798\n",
      " 3047/5000: episode: 228, duration: 0.101s, episode steps: 20, steps per second: 199, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.365, 0.922], loss: 2.716566, mean_absolute_error: 6.431883, mean_q: 11.829265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3068/5000: episode: 229, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.086 [-0.585, 0.874], loss: 3.930601, mean_absolute_error: 6.442440, mean_q: 11.671156\n",
      " 3095/5000: episode: 230, duration: 0.080s, episode steps: 27, steps per second: 339, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.096 [-0.429, 0.676], loss: 2.869599, mean_absolute_error: 6.456454, mean_q: 11.815352\n",
      " 3123/5000: episode: 231, duration: 0.075s, episode steps: 28, steps per second: 373, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.225, 0.970], loss: 3.479470, mean_absolute_error: 6.519836, mean_q: 11.895141\n",
      " 3191/5000: episode: 232, duration: 0.191s, episode steps: 68, steps per second: 356, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.186 [-0.563, 1.120], loss: 3.172863, mean_absolute_error: 6.498971, mean_q: 11.877181\n",
      " 3224/5000: episode: 233, duration: 0.100s, episode steps: 33, steps per second: 329, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.116 [-0.907, 0.188], loss: 3.384941, mean_absolute_error: 6.506608, mean_q: 11.828418\n",
      " 3263/5000: episode: 234, duration: 0.103s, episode steps: 39, steps per second: 378, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.115 [-0.610, 0.344], loss: 3.043416, mean_absolute_error: 6.529461, mean_q: 11.975760\n",
      " 3299/5000: episode: 235, duration: 0.101s, episode steps: 36, steps per second: 358, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.097 [-0.857, 0.580], loss: 2.970857, mean_absolute_error: 6.670725, mean_q: 12.304142\n",
      " 3344/5000: episode: 236, duration: 0.124s, episode steps: 45, steps per second: 364, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.111 [-0.652, 0.348], loss: 2.722577, mean_absolute_error: 6.504945, mean_q: 12.006730\n",
      " 3387/5000: episode: 237, duration: 0.139s, episode steps: 43, steps per second: 308, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.084 [-0.866, 0.567], loss: 2.868873, mean_absolute_error: 6.565001, mean_q: 12.104319\n",
      " 3446/5000: episode: 238, duration: 0.171s, episode steps: 59, steps per second: 346, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.046 [-0.628, 0.419], loss: 3.135935, mean_absolute_error: 6.594312, mean_q: 12.081697\n",
      " 3483/5000: episode: 239, duration: 0.119s, episode steps: 37, steps per second: 311, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.104 [-0.637, 0.367], loss: 2.982409, mean_absolute_error: 6.650799, mean_q: 12.206740\n",
      " 3541/5000: episode: 240, duration: 0.173s, episode steps: 58, steps per second: 335, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.053 [-0.812, 0.384], loss: 3.062300, mean_absolute_error: 6.652602, mean_q: 12.196856\n",
      " 3577/5000: episode: 241, duration: 0.109s, episode steps: 36, steps per second: 330, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.111 [-0.825, 0.380], loss: 2.738225, mean_absolute_error: 6.712178, mean_q: 12.384026\n",
      " 3618/5000: episode: 242, duration: 0.111s, episode steps: 41, steps per second: 370, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.125 [-0.652, 0.198], loss: 2.772763, mean_absolute_error: 6.663801, mean_q: 12.330673\n",
      " 3756/5000: episode: 243, duration: 0.392s, episode steps: 138, steps per second: 352, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.016 [-1.032, 0.414], loss: 2.697228, mean_absolute_error: 6.764613, mean_q: 12.575015\n",
      " 3793/5000: episode: 244, duration: 0.098s, episode steps: 37, steps per second: 376, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.117 [-0.767, 0.376], loss: 3.327414, mean_absolute_error: 6.898606, mean_q: 12.731273\n",
      " 3830/5000: episode: 245, duration: 0.093s, episode steps: 37, steps per second: 396, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.140 [-0.634, 0.161], loss: 2.535445, mean_absolute_error: 6.793482, mean_q: 12.650949\n",
      " 3888/5000: episode: 246, duration: 0.179s, episode steps: 58, steps per second: 324, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.093 [-0.708, 0.400], loss: 2.830473, mean_absolute_error: 6.960672, mean_q: 12.937593\n",
      " 3919/5000: episode: 247, duration: 0.143s, episode steps: 31, steps per second: 216, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.124 [-0.852, 0.404], loss: 2.090541, mean_absolute_error: 6.864172, mean_q: 12.931082\n",
      " 3974/5000: episode: 248, duration: 0.166s, episode steps: 55, steps per second: 332, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.089 [-0.821, 0.414], loss: 2.411689, mean_absolute_error: 6.992675, mean_q: 13.163488\n",
      " 4023/5000: episode: 249, duration: 0.141s, episode steps: 49, steps per second: 347, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.104 [-0.641, 0.211], loss: 2.832305, mean_absolute_error: 7.083289, mean_q: 13.276299\n",
      " 4101/5000: episode: 250, duration: 0.237s, episode steps: 78, steps per second: 329, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.023 [-0.774, 0.616], loss: 2.706174, mean_absolute_error: 7.105821, mean_q: 13.279090\n",
      " 4130/5000: episode: 251, duration: 0.086s, episode steps: 29, steps per second: 338, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.122 [-0.790, 0.393], loss: 2.557902, mean_absolute_error: 7.095123, mean_q: 13.251324\n",
      " 4167/5000: episode: 252, duration: 0.114s, episode steps: 37, steps per second: 324, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.098 [-0.634, 0.186], loss: 2.879308, mean_absolute_error: 7.187574, mean_q: 13.441301\n",
      " 4214/5000: episode: 253, duration: 0.138s, episode steps: 47, steps per second: 340, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.094 [-0.582, 0.405], loss: 3.360306, mean_absolute_error: 7.222504, mean_q: 13.409682\n",
      " 4252/5000: episode: 254, duration: 0.105s, episode steps: 38, steps per second: 361, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.087 [-0.739, 0.403], loss: 3.085077, mean_absolute_error: 7.281330, mean_q: 13.576801\n",
      " 4282/5000: episode: 255, duration: 0.077s, episode steps: 30, steps per second: 387, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.120 [-0.751, 0.381], loss: 2.615039, mean_absolute_error: 7.251284, mean_q: 13.603254\n",
      " 4321/5000: episode: 256, duration: 0.113s, episode steps: 39, steps per second: 347, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.116 [-0.618, 0.363], loss: 2.258900, mean_absolute_error: 7.246954, mean_q: 13.655212\n",
      " 4359/5000: episode: 257, duration: 0.097s, episode steps: 38, steps per second: 391, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.107 [-0.793, 0.184], loss: 3.023099, mean_absolute_error: 7.414409, mean_q: 13.907534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4398/5000: episode: 258, duration: 0.102s, episode steps: 39, steps per second: 384, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.106 [-0.909, 0.565], loss: 2.532679, mean_absolute_error: 7.364540, mean_q: 13.858647\n",
      " 4429/5000: episode: 259, duration: 0.088s, episode steps: 31, steps per second: 354, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.121 [-0.832, 0.405], loss: 2.437815, mean_absolute_error: 7.377513, mean_q: 13.931731\n",
      " 4473/5000: episode: 260, duration: 0.137s, episode steps: 44, steps per second: 321, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.093 [-0.706, 0.225], loss: 3.513305, mean_absolute_error: 7.448384, mean_q: 13.881970\n",
      " 4500/5000: episode: 261, duration: 0.086s, episode steps: 27, steps per second: 314, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.137 [-0.786, 0.206], loss: 2.313752, mean_absolute_error: 7.476992, mean_q: 14.105200\n",
      " 4539/5000: episode: 262, duration: 0.109s, episode steps: 39, steps per second: 358, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.121 [-0.596, 0.386], loss: 2.663009, mean_absolute_error: 7.471907, mean_q: 14.084599\n",
      " 4570/5000: episode: 263, duration: 0.099s, episode steps: 31, steps per second: 313, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.097 [-0.864, 0.409], loss: 2.615836, mean_absolute_error: 7.543748, mean_q: 14.249446\n",
      " 4599/5000: episode: 264, duration: 0.084s, episode steps: 29, steps per second: 346, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.130 [-0.603, 0.215], loss: 2.515262, mean_absolute_error: 7.608200, mean_q: 14.399438\n",
      " 4644/5000: episode: 265, duration: 0.119s, episode steps: 45, steps per second: 379, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.102 [-0.596, 0.350], loss: 2.707500, mean_absolute_error: 7.592908, mean_q: 14.301104\n",
      " 4696/5000: episode: 266, duration: 0.144s, episode steps: 52, steps per second: 361, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.091 [-0.983, 0.391], loss: 2.323467, mean_absolute_error: 7.662208, mean_q: 14.566396\n",
      " 4739/5000: episode: 267, duration: 0.110s, episode steps: 43, steps per second: 389, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.119 [-0.648, 0.191], loss: 2.732769, mean_absolute_error: 7.734716, mean_q: 14.675014\n",
      " 4786/5000: episode: 268, duration: 0.130s, episode steps: 47, steps per second: 360, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.086 [-0.618, 0.216], loss: 2.728395, mean_absolute_error: 7.754776, mean_q: 14.686072\n",
      " 4835/5000: episode: 269, duration: 0.165s, episode steps: 49, steps per second: 298, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.078 [-0.611, 0.409], loss: 3.372175, mean_absolute_error: 7.948708, mean_q: 15.009847\n",
      " 4881/5000: episode: 270, duration: 0.126s, episode steps: 46, steps per second: 365, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.083 [-0.799, 0.383], loss: 2.826975, mean_absolute_error: 7.873487, mean_q: 14.923263\n",
      " 4912/5000: episode: 271, duration: 0.080s, episode steps: 31, steps per second: 387, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.118 [-0.865, 0.441], loss: 3.515077, mean_absolute_error: 7.905561, mean_q: 14.835787\n",
      " 4945/5000: episode: 272, duration: 0.088s, episode steps: 33, steps per second: 376, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.109 [-0.816, 0.399], loss: 2.797776, mean_absolute_error: 7.928813, mean_q: 15.030492\n",
      " 4986/5000: episode: 273, duration: 0.113s, episode steps: 41, steps per second: 362, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.100 [-0.601, 0.431], loss: 3.692938, mean_absolute_error: 8.067242, mean_q: 15.142522\n",
      "done, took 16.405 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c9ae8732b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to learn something! There are four settings you can consider changing, however, only one which has an effect on your training performance:\n",
    "\n",
    "* nb_steps: the larger, the more time your bot gets for trying to find a good strategy, but the longer you'll have to wait.\n",
    "* verbose: printing running status. 0 for no logging, 1 for interval logging, 2 for episode logging\n",
    "* visualize: you can visualize the training for show, but this mostly slows down training\n",
    "* log_interval: if verbose=1, the number of steps that are considered to be an interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=5000, verbose=2, visualize=False, log_interval=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code to test your DRL model. The larger the reward and number of steps per episode, the better your model performs. Running about 10 episodes will give you a proper overall status.\n",
    "\n",
    "NOTE: Don't close the graph after/while running it. This will reset the kernel and cause you having to re-run everything. You can simply re-run the below code instead, each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 46.000, steps: 46\n",
      "Episode 2: reward: 29.000, steps: 29\n",
      "Episode 3: reward: 45.000, steps: 45\n",
      "Episode 4: reward: 47.000, steps: 47\n",
      "Episode 5: reward: 50.000, steps: 50\n",
      "Episode 6: reward: 33.000, steps: 33\n",
      "Episode 7: reward: 29.000, steps: 29\n",
      "Episode 8: reward: 43.000, steps: 43\n",
      "Episode 9: reward: 39.000, steps: 39\n",
      "Episode 10: reward: 41.000, steps: 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c928b77198>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
