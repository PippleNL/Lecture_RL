Training for 5000 steps ...
WARNING:tensorflow:From C:\Users\LennartvanHam\Anaconda3\envs\CartPole\lib\site-packages\keras-2.3.1-py3.7.egg\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

    9/5000: episode: 1, duration: 0.101s, episode steps: 9, steps per second: 89, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.882, 1.749], loss: --, mae: --, mean_q: --
   20/5000: episode: 2, duration: 0.828s, episode steps: 11, steps per second: 13, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.088 [-2.715, 1.807], loss: 0.681350, mae: 0.979550, mean_q: 0.845577
   29/5000: episode: 3, duration: 0.076s, episode steps: 9, steps per second: 119, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.111 [-2.226, 1.423], loss: 0.771551, mae: 0.991267, mean_q: 1.026172
   39/5000: episode: 4, duration: 0.075s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.152 [-2.770, 1.738], loss: 0.695907, mae: 0.902907, mean_q: 1.104702
   47/5000: episode: 5, duration: 0.112s, episode steps: 8, steps per second: 72, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.563, 1.604], loss: 0.723157, mae: 0.885468, mean_q: 1.231077
   56/5000: episode: 6, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.762, 1.768], loss: 0.546322, mae: 0.785605, mean_q: 1.397401
   65/5000: episode: 7, duration: 0.138s, episode steps: 9, steps per second: 65, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.798, 1.804], loss: 0.552150, mae: 0.734417, mean_q: 1.443396
   74/5000: episode: 8, duration: 0.082s, episode steps: 9, steps per second: 109, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.817, 1.757], loss: 0.514400, mae: 0.686315, mean_q: 1.583709
   83/5000: episode: 9, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-2.741, 1.805], loss: 0.534434, mae: 0.640773, mean_q: 1.633211
   94/5000: episode: 10, duration: 0.094s, episode steps: 11, steps per second: 117, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.124 [-2.719, 1.743], loss: 0.532404, mae: 0.619150, mean_q: 1.690027
  103/5000: episode: 11, duration: 0.071s, episode steps: 9, steps per second: 127, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.802, 1.734], loss: 0.518572, mae: 0.572723, mean_q: 1.752221
  112/5000: episode: 12, duration: 0.167s, episode steps: 9, steps per second: 54, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.795, 1.802], loss: 0.587661, mae: 0.603994, mean_q: 1.827226
  121/5000: episode: 13, duration: 0.078s, episode steps: 9, steps per second: 116, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.881, 1.804], loss: 0.558413, mae: 0.551617, mean_q: 1.953104
  130/5000: episode: 14, duration: 0.077s, episode steps: 9, steps per second: 117, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.812, 1.778], loss: 0.515376, mae: 0.493245, mean_q: 1.930567
  140/5000: episode: 15, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.069, 1.948], loss: 0.610721, mae: 0.552268, mean_q: 2.045455
  150/5000: episode: 16, duration: 0.082s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.016, 1.962], loss: 0.671048, mae: 0.551816, mean_q: 2.108243
  159/5000: episode: 17, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.873, 1.780], loss: 0.619804, mae: 0.564491, mean_q: 2.134645
  168/5000: episode: 18, duration: 0.071s, episode steps: 9, steps per second: 126, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.835, 1.751], loss: 0.611291, mae: 0.595237, mean_q: 2.173026
  179/5000: episode: 19, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.092 [-2.774, 1.785], loss: 0.606465, mae: 0.611506, mean_q: 2.231355
  189/5000: episode: 20, duration: 0.077s, episode steps: 10, steps per second: 129, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.091, 1.961], loss: 0.572806, mae: 0.647787, mean_q: 2.261617
  199/5000: episode: 21, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.105 [-2.979, 1.974], loss: 0.463446, mae: 0.612332, mean_q: 2.357069
  208/5000: episode: 22, duration: 0.121s, episode steps: 9, steps per second: 74, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.743, 1.748], loss: 0.519794, mae: 0.645328, mean_q: 2.545944
  219/5000: episode: 23, duration: 0.087s, episode steps: 11, steps per second: 126, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.741, 1.796], loss: 0.586248, mae: 0.691962, mean_q: 2.565465
  229/5000: episode: 24, duration: 0.075s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.099, 1.970], loss: 0.643362, mae: 0.790050, mean_q: 2.690190
  238/5000: episode: 25, duration: 0.068s, episode steps: 9, steps per second: 132, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.861, 1.748], loss: 0.578931, mae: 0.778365, mean_q: 2.646034
  248/5000: episode: 26, duration: 0.075s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.984, 1.969], loss: 0.597304, mae: 0.810816, mean_q: 2.713845
  257/5000: episode: 27, duration: 0.070s, episode steps: 9, steps per second: 129, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.175 [-2.889, 1.752], loss: 0.709669, mae: 0.867936, mean_q: 2.741120
  267/5000: episode: 28, duration: 0.078s, episode steps: 10, steps per second: 129, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.026, 1.906], loss: 0.671288, mae: 0.896425, mean_q: 2.771172
  276/5000: episode: 29, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-2.791, 1.807], loss: 0.675582, mae: 0.924164, mean_q: 2.798165
  285/5000: episode: 30, duration: 0.079s, episode steps: 9, steps per second: 114, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.760, 1.804], loss: 0.537545, mae: 0.897537, mean_q: 2.801497
  293/5000: episode: 31, duration: 0.056s, episode steps: 8, steps per second: 143, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.506, 1.535], loss: 0.598050, mae: 0.923176, mean_q: 2.941660
  306/5000: episode: 32, duration: 0.139s, episode steps: 13, steps per second: 94, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.119 [-2.756, 1.716], loss: 0.576229, mae: 0.944622, mean_q: 3.080331
  316/5000: episode: 33, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.551, 1.590], loss: 0.663513, mae: 0.984263, mean_q: 3.061838
  326/5000: episode: 34, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.067, 2.005], loss: 0.752486, mae: 1.084840, mean_q: 3.080610
  337/5000: episode: 35, duration: 0.083s, episode steps: 11, steps per second: 132, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.134 [-2.740, 1.737], loss: 0.626237, mae: 1.063907, mean_q: 3.036711
  346/5000: episode: 36, duration: 0.086s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.825, 1.740], loss: 0.688915, mae: 1.123300, mean_q: 3.186813
  355/5000: episode: 37, duration: 0.066s, episode steps: 9, steps per second: 137, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.291, 1.390], loss: 0.538006, mae: 1.100460, mean_q: 3.208615
  365/5000: episode: 38, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.004, 1.973], loss: 0.606932, mae: 1.126719, mean_q: 3.324267
  373/5000: episode: 39, duration: 0.103s, episode steps: 8, steps per second: 78, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.490, 1.562], loss: 0.612106, mae: 1.167350, mean_q: 3.293755
  385/5000: episode: 40, duration: 0.094s, episode steps: 12, steps per second: 127, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.125 [-3.069, 1.942], loss: 0.510120, mae: 1.185916, mean_q: 3.331288
  396/5000: episode: 41, duration: 0.131s, episode steps: 11, steps per second: 84, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-3.302, 2.151], loss: 0.691454, mae: 1.295938, mean_q: 3.443354
  407/5000: episode: 42, duration: 0.077s, episode steps: 11, steps per second: 144, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.149 [-2.835, 1.714], loss: 0.568110, mae: 1.326306, mean_q: 3.473413
  417/5000: episode: 43, duration: 0.074s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.095 [-2.608, 1.794], loss: 0.570391, mae: 1.368773, mean_q: 3.577864
  426/5000: episode: 44, duration: 0.083s, episode steps: 9, steps per second: 108, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.830, 1.801], loss: 0.622525, mae: 1.413319, mean_q: 3.621201
  439/5000: episode: 45, duration: 0.086s, episode steps: 13, steps per second: 151, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.078 [-2.742, 1.800], loss: 0.563291, mae: 1.471319, mean_q: 3.643625
  449/5000: episode: 46, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.102 [-2.652, 1.793], loss: 0.482431, mae: 1.474101, mean_q: 3.702026
  459/5000: episode: 47, duration: 0.074s, episode steps: 10, steps per second: 136, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.112, 1.995], loss: 0.551249, mae: 1.522696, mean_q: 3.836028
  469/5000: episode: 48, duration: 0.081s, episode steps: 10, steps per second: 124, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-3.107, 1.923], loss: 0.545603, mae: 1.558816, mean_q: 3.873908
  479/5000: episode: 49, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.103, 1.949], loss: 0.545430, mae: 1.553143, mean_q: 3.831245
  489/5000: episode: 50, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.152 [-2.256, 1.351], loss: 0.589624, mae: 1.614630, mean_q: 3.853639
  498/5000: episode: 51, duration: 0.067s, episode steps: 9, steps per second: 133, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.133 [-2.216, 1.406], loss: 0.511573, mae: 1.586516, mean_q: 3.942703
  506/5000: episode: 52, duration: 0.151s, episode steps: 8, steps per second: 53, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.537, 1.534], loss: 0.506562, mae: 1.614387, mean_q: 3.983403
  514/5000: episode: 53, duration: 0.070s, episode steps: 8, steps per second: 115, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.526, 1.526], loss: 0.515987, mae: 1.675701, mean_q: 4.072682
  524/5000: episode: 54, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-3.135, 1.952], loss: 0.594791, mae: 1.729343, mean_q: 4.124402
  534/5000: episode: 55, duration: 0.077s, episode steps: 10, steps per second: 130, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.083, 1.975], loss: 0.474494, mae: 1.729399, mean_q: 4.131312
  543/5000: episode: 56, duration: 0.083s, episode steps: 9, steps per second: 109, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.154 [-2.157, 1.340], loss: 0.462808, mae: 1.759503, mean_q: 4.242135
  553/5000: episode: 57, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.141 [-2.513, 1.553], loss: 0.500731, mae: 1.825849, mean_q: 4.319010
  564/5000: episode: 58, duration: 0.080s, episode steps: 11, steps per second: 137, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.112 [-2.774, 1.790], loss: 0.516299, mae: 1.844519, mean_q: 4.327883
  574/5000: episode: 59, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.966, 1.915], loss: 0.466505, mae: 1.883726, mean_q: 4.294561
  583/5000: episode: 60, duration: 0.071s, episode steps: 9, steps per second: 127, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.807, 1.720], loss: 0.486285, mae: 1.919581, mean_q: 4.356607
  595/5000: episode: 61, duration: 0.165s, episode steps: 12, steps per second: 73, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.128 [-2.579, 1.556], loss: 0.421619, mae: 1.918570, mean_q: 4.422708
  604/5000: episode: 62, duration: 0.110s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.818, 1.767], loss: 0.452066, mae: 1.939408, mean_q: 4.588021
  613/5000: episode: 63, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.358, 1.416], loss: 0.350917, mae: 1.923681, mean_q: 4.658312
  622/5000: episode: 64, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.831, 1.804], loss: 0.564078, mae: 2.018675, mean_q: 4.568723
  630/5000: episode: 65, duration: 0.120s, episode steps: 8, steps per second: 67, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.523, 1.552], loss: 0.429062, mae: 2.020933, mean_q: 4.563897
  640/5000: episode: 66, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.138 [-2.584, 1.586], loss: 0.437449, mae: 2.053577, mean_q: 4.584777
  648/5000: episode: 67, duration: 0.105s, episode steps: 8, steps per second: 76, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.524, 1.562], loss: 0.403664, mae: 2.078145, mean_q: 4.694934
  658/5000: episode: 68, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.709, 1.780], loss: 0.464923, mae: 2.133634, mean_q: 4.746989
  667/5000: episode: 69, duration: 0.086s, episode steps: 9, steps per second: 104, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.251, 1.386], loss: 0.462885, mae: 2.175541, mean_q: 4.673485
  679/5000: episode: 70, duration: 0.127s, episode steps: 12, steps per second: 95, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.094 [-2.225, 1.388], loss: 0.455090, mae: 2.199899, mean_q: 4.676145
  690/5000: episode: 71, duration: 0.106s, episode steps: 11, steps per second: 104, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.123 [-2.346, 1.517], loss: 0.427830, mae: 2.209880, mean_q: 4.792002
  700/5000: episode: 72, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.112 [-2.629, 1.787], loss: 0.334185, mae: 2.181921, mean_q: 4.901958
  709/5000: episode: 73, duration: 0.094s, episode steps: 9, steps per second: 96, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.444, 1.521], loss: 0.423998, mae: 2.246667, mean_q: 4.980252
  720/5000: episode: 74, duration: 0.101s, episode steps: 11, steps per second: 109, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.106 [-2.322, 1.596], loss: 0.380033, mae: 2.234756, mean_q: 4.947281
  730/5000: episode: 75, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.131 [-2.425, 1.585], loss: 0.374105, mae: 2.246926, mean_q: 5.002114
  743/5000: episode: 76, duration: 0.121s, episode steps: 13, steps per second: 108, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.063 [-2.410, 1.607], loss: 0.359357, mae: 2.265143, mean_q: 5.050440
  751/5000: episode: 77, duration: 0.130s, episode steps: 8, steps per second: 61, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.143 [-2.223, 1.395], loss: 0.357888, mae: 2.301904, mean_q: 5.177393
  762/5000: episode: 78, duration: 0.083s, episode steps: 11, steps per second: 133, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.109 [-2.353, 1.603], loss: 0.384871, mae: 2.277843, mean_q: 5.058908
  773/5000: episode: 79, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.116 [-2.466, 1.600], loss: 0.371618, mae: 2.268369, mean_q: 5.088756
  783/5000: episode: 80, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.129 [-2.199, 1.414], loss: 0.381740, mae: 2.291449, mean_q: 5.129332
  792/5000: episode: 81, duration: 0.083s, episode steps: 9, steps per second: 109, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.122 [-2.410, 1.577], loss: 0.318809, mae: 2.274907, mean_q: 5.291000
  802/5000: episode: 82, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.123 [-2.006, 1.222], loss: 0.356936, mae: 2.287315, mean_q: 5.300227
  811/5000: episode: 83, duration: 0.095s, episode steps: 9, steps per second: 94, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.216, 1.349], loss: 0.350079, mae: 2.296594, mean_q: 5.259105
  820/5000: episode: 84, duration: 0.120s, episode steps: 9, steps per second: 75, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.457, 1.609], loss: 0.333827, mae: 2.299014, mean_q: 5.255648
  832/5000: episode: 85, duration: 0.090s, episode steps: 12, steps per second: 133, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.107 [-2.409, 1.553], loss: 0.352065, mae: 2.309432, mean_q: 5.272009
  842/5000: episode: 86, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.141 [-2.452, 1.574], loss: 0.411647, mae: 2.369877, mean_q: 5.264460
  851/5000: episode: 87, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.131 [-2.173, 1.417], loss: 0.333239, mae: 2.362986, mean_q: 5.337735
  862/5000: episode: 88, duration: 0.113s, episode steps: 11, steps per second: 98, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.129 [-2.329, 1.342], loss: 0.272782, mae: 2.367762, mean_q: 5.462259
  871/5000: episode: 89, duration: 0.085s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.461, 1.541], loss: 0.299546, mae: 2.404863, mean_q: 5.551255
  880/5000: episode: 90, duration: 0.063s, episode steps: 9, steps per second: 143, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.153 [-2.531, 1.607], loss: 0.347286, mae: 2.430184, mean_q: 5.475558
  894/5000: episode: 91, duration: 0.120s, episode steps: 14, steps per second: 117, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.097 [-2.127, 1.233], loss: 0.342901, mae: 2.440946, mean_q: 5.393225
  903/5000: episode: 92, duration: 0.085s, episode steps: 9, steps per second: 106, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.136 [-2.160, 1.347], loss: 0.264673, mae: 2.447856, mean_q: 5.637908
  913/5000: episode: 93, duration: 0.158s, episode steps: 10, steps per second: 63, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.134 [-2.242, 1.370], loss: 0.321838, mae: 2.458116, mean_q: 5.403865
  922/5000: episode: 94, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.136 [-2.125, 1.347], loss: 0.344770, mae: 2.518756, mean_q: 5.481603
  933/5000: episode: 95, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.091 [-2.264, 1.596], loss: 0.261985, mae: 2.462775, mean_q: 5.438866
  943/5000: episode: 96, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.114 [-2.201, 1.417], loss: 0.352258, mae: 2.555279, mean_q: 5.614406
  951/5000: episode: 97, duration: 0.125s, episode steps: 8, steps per second: 64, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.142 [-2.168, 1.356], loss: 0.313136, mae: 2.509676, mean_q: 5.534689
  962/5000: episode: 98, duration: 0.103s, episode steps: 11, steps per second: 107, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.108 [-2.044, 1.395], loss: 0.367772, mae: 2.588074, mean_q: 5.607101
  972/5000: episode: 99, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.367, 1.553], loss: 0.299879, mae: 2.529372, mean_q: 5.575856
  981/5000: episode: 100, duration: 0.075s, episode steps: 9, steps per second: 120, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.182 [-2.887, 1.722], loss: 0.311083, mae: 2.564528, mean_q: 5.650642
  990/5000: episode: 101, duration: 0.108s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.162 [-2.526, 1.574], loss: 0.250199, mae: 2.553717, mean_q: 5.751636
 1002/5000: episode: 102, duration: 0.135s, episode steps: 12, steps per second: 89, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.111 [-2.277, 1.363], loss: 0.236251, mae: 2.596350, mean_q: 5.861366
 1012/5000: episode: 103, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.109 [-2.006, 1.188], loss: 0.274685, mae: 2.571733, mean_q: 5.670996
 1022/5000: episode: 104, duration: 0.177s, episode steps: 10, steps per second: 56, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.123 [-2.351, 1.563], loss: 0.216094, mae: 2.604109, mean_q: 5.732497
 1031/5000: episode: 105, duration: 0.135s, episode steps: 9, steps per second: 67, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.149 [-2.203, 1.354], loss: 0.238833, mae: 2.641813, mean_q: 5.814220
 1040/5000: episode: 106, duration: 0.127s, episode steps: 9, steps per second: 71, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.145 [-2.168, 1.363], loss: 0.283733, mae: 2.677121, mean_q: 5.778231
 1049/5000: episode: 107, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.127 [-2.456, 1.580], loss: 0.260074, mae: 2.680257, mean_q: 5.742322
 1059/5000: episode: 108, duration: 0.106s, episode steps: 10, steps per second: 95, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.102 [-1.867, 1.219], loss: 0.220780, mae: 2.691837, mean_q: 5.713679
 1070/5000: episode: 109, duration: 0.105s, episode steps: 11, steps per second: 104, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.104 [-2.277, 1.594], loss: 0.218817, mae: 2.739548, mean_q: 5.839459
 1080/5000: episode: 110, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-1.945, 1.180], loss: 0.232421, mae: 2.765662, mean_q: 5.812831
 1091/5000: episode: 111, duration: 0.097s, episode steps: 11, steps per second: 113, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.123 [-1.839, 1.163], loss: 0.281102, mae: 2.746255, mean_q: 5.646037
 1102/5000: episode: 112, duration: 0.112s, episode steps: 11, steps per second: 98, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.108 [-1.886, 1.195], loss: 0.245280, mae: 2.794145, mean_q: 5.797683
 1112/5000: episode: 113, duration: 0.137s, episode steps: 10, steps per second: 73, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.102 [-2.100, 1.394], loss: 0.177516, mae: 2.931749, mean_q: 6.216469
 1123/5000: episode: 114, duration: 0.079s, episode steps: 11, steps per second: 138, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.120 [-2.017, 1.331], loss: 0.250489, mae: 2.951229, mean_q: 6.086976
 1134/5000: episode: 115, duration: 0.121s, episode steps: 11, steps per second: 91, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.124 [-2.059, 1.331], loss: 0.237132, mae: 2.923076, mean_q: 6.005436
 1145/5000: episode: 116, duration: 0.098s, episode steps: 11, steps per second: 112, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.104 [-1.835, 1.146], loss: 0.190524, mae: 2.992675, mean_q: 6.167485
 1154/5000: episode: 117, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.139 [-1.913, 1.184], loss: 0.241224, mae: 2.966139, mean_q: 6.038948
 1163/5000: episode: 118, duration: 0.120s, episode steps: 9, steps per second: 75, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.135 [-1.848, 1.149], loss: 0.270261, mae: 2.953451, mean_q: 5.977552
 1171/5000: episode: 119, duration: 0.090s, episode steps: 8, steps per second: 89, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.142 [-1.902, 1.185], loss: 0.184417, mae: 2.887463, mean_q: 5.884422
 1179/5000: episode: 120, duration: 0.059s, episode steps: 8, steps per second: 136, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.130 [-1.896, 1.219], loss: 0.306862, mae: 2.936636, mean_q: 5.905037
 1188/5000: episode: 121, duration: 0.119s, episode steps: 9, steps per second: 75, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.157 [-2.173, 1.334], loss: 0.243589, mae: 2.984678, mean_q: 6.129757
 1200/5000: episode: 122, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.093 [-1.770, 1.159], loss: 0.265098, mae: 2.913667, mean_q: 5.915040
 1208/5000: episode: 123, duration: 0.114s, episode steps: 8, steps per second: 70, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.528, 1.529], loss: 0.299205, mae: 2.955833, mean_q: 5.976958
 1219/5000: episode: 124, duration: 0.081s, episode steps: 11, steps per second: 136, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.111 [-2.039, 1.367], loss: 0.231493, mae: 3.003653, mean_q: 6.119116
 1229/5000: episode: 125, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.122 [-1.862, 1.170], loss: 0.220690, mae: 2.976605, mean_q: 6.061401
 1237/5000: episode: 126, duration: 0.103s, episode steps: 8, steps per second: 77, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.142 [-1.912, 1.204], loss: 0.152342, mae: 2.980252, mean_q: 6.110692
 1246/5000: episode: 127, duration: 0.068s, episode steps: 9, steps per second: 132, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.126 [-1.899, 1.210], loss: 0.268317, mae: 3.040507, mean_q: 6.153696
 1257/5000: episode: 128, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.098 [-2.038, 1.412], loss: 0.230713, mae: 3.045105, mean_q: 6.158589
 1268/5000: episode: 129, duration: 0.091s, episode steps: 11, steps per second: 121, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.128 [-2.345, 1.543], loss: 0.173860, mae: 3.003132, mean_q: 6.105423
 1278/5000: episode: 130, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.115 [-2.353, 1.540], loss: 0.186694, mae: 3.129082, mean_q: 6.405113
 1287/5000: episode: 131, duration: 0.117s, episode steps: 9, steps per second: 77, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.145 [-2.211, 1.353], loss: 0.233421, mae: 2.981718, mean_q: 5.975759
 1298/5000: episode: 132, duration: 0.110s, episode steps: 11, steps per second: 100, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.090 [-1.785, 1.187], loss: 0.249602, mae: 3.067020, mean_q: 6.130518
 1308/5000: episode: 133, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.114 [-2.138, 1.399], loss: 0.199018, mae: 3.125154, mean_q: 6.253484
 1319/5000: episode: 134, duration: 0.102s, episode steps: 11, steps per second: 108, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.109 [-1.545, 1.001], loss: 0.193625, mae: 3.068845, mean_q: 6.100401
 1330/5000: episode: 135, duration: 0.081s, episode steps: 11, steps per second: 136, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.120 [-1.575, 0.964], loss: 0.154793, mae: 3.147403, mean_q: 6.274101
 1346/5000: episode: 136, duration: 0.163s, episode steps: 16, steps per second: 98, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.522, 0.937], loss: 0.233048, mae: 3.219256, mean_q: 6.341160
 1359/5000: episode: 137, duration: 0.186s, episode steps: 13, steps per second: 70, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.109 [-1.543, 0.981], loss: 0.210357, mae: 3.171573, mean_q: 6.214500
 1389/5000: episode: 138, duration: 0.188s, episode steps: 30, steps per second: 160, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.137 [-1.342, 0.944], loss: 0.224049, mae: 3.225018, mean_q: 6.247287
 1411/5000: episode: 139, duration: 0.182s, episode steps: 22, steps per second: 121, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.227 [0.000, 1.000], mean observation: 0.020 [-2.373, 3.304], loss: 0.214518, mae: 3.367554, mean_q: 6.475561
 1421/5000: episode: 140, duration: 0.089s, episode steps: 10, steps per second: 112, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.950, 3.025], loss: 0.662415, mae: 3.501528, mean_q: 6.630294
 1430/5000: episode: 141, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.135 [-1.585, 2.452], loss: 0.982904, mae: 3.466440, mean_q: 6.550849
 1453/5000: episode: 142, duration: 0.190s, episode steps: 23, steps per second: 121, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.132 [-0.952, 0.575], loss: 0.652240, mae: 3.490119, mean_q: 6.583962
 1477/5000: episode: 143, duration: 0.247s, episode steps: 24, steps per second: 97, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.084 [-1.319, 2.055], loss: 0.696056, mae: 3.551749, mean_q: 6.717749
 1559/5000: episode: 144, duration: 0.539s, episode steps: 82, steps per second: 152, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.148 [-1.819, 1.516], loss: 0.419009, mae: 3.638703, mean_q: 6.943282
 1568/5000: episode: 145, duration: 0.071s, episode steps: 9, steps per second: 126, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [-1.743, 2.893], loss: 0.502323, mae: 3.867753, mean_q: 7.328759
 1578/5000: episode: 146, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.098 [-1.776, 2.613], loss: 0.768813, mae: 3.769225, mean_q: 7.123400
 1588/5000: episode: 147, duration: 0.074s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.119 [-1.601, 2.496], loss: 0.659444, mae: 3.803099, mean_q: 7.251393
 1598/5000: episode: 148, duration: 0.078s, episode steps: 10, steps per second: 129, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.151 [-1.566, 2.583], loss: 0.239718, mae: 3.858772, mean_q: 7.457288
 1609/5000: episode: 149, duration: 0.106s, episode steps: 11, steps per second: 103, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.132 [-1.755, 2.739], loss: 0.552823, mae: 3.909073, mean_q: 7.468364
 1619/5000: episode: 150, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.965, 3.074], loss: 1.451451, mae: 4.156878, mean_q: 7.846803
 1628/5000: episode: 151, duration: 0.097s, episode steps: 9, steps per second: 93, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.758, 2.764], loss: 1.147359, mae: 4.031876, mean_q: 7.661173
 1637/5000: episode: 152, duration: 0.067s, episode steps: 9, steps per second: 135, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.130 [-1.809, 2.843], loss: 0.700580, mae: 4.091136, mean_q: 7.835876
 1646/5000: episode: 153, duration: 0.133s, episode steps: 9, steps per second: 67, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.754, 2.803], loss: 1.140025, mae: 4.142347, mean_q: 7.874737
 1655/5000: episode: 154, duration: 0.090s, episode steps: 9, steps per second: 100, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.784, 2.752], loss: 0.562666, mae: 4.088549, mean_q: 7.765384
 1666/5000: episode: 155, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.123 [-1.750, 2.790], loss: 1.804265, mae: 4.295520, mean_q: 8.038715
 1677/5000: episode: 156, duration: 0.084s, episode steps: 11, steps per second: 131, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.121 [-1.780, 2.792], loss: 2.167601, mae: 4.308888, mean_q: 8.049898
 1685/5000: episode: 157, duration: 0.125s, episode steps: 8, steps per second: 64, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.590, 2.609], loss: 0.439612, mae: 4.227572, mean_q: 8.056967
 1695/5000: episode: 158, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.134 [-1.556, 2.408], loss: 1.988781, mae: 4.380959, mean_q: 8.096914
 1706/5000: episode: 159, duration: 0.104s, episode steps: 11, steps per second: 106, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.124 [-1.151, 2.009], loss: 1.268672, mae: 4.365467, mean_q: 8.166864
 1718/5000: episode: 160, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.119 [-1.315, 2.047], loss: 2.713633, mae: 4.540823, mean_q: 8.332804
 1726/5000: episode: 161, duration: 0.095s, episode steps: 8, steps per second: 84, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.148 [-1.185, 1.912], loss: 2.007228, mae: 4.388871, mean_q: 8.076200
 1735/5000: episode: 162, duration: 0.089s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.157 [-1.154, 1.897], loss: 2.289969, mae: 4.614108, mean_q: 8.493627
 1760/5000: episode: 163, duration: 0.233s, episode steps: 25, steps per second: 107, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.127 [-1.017, 1.880], loss: 1.578225, mae: 4.481582, mean_q: 8.226407
 1774/5000: episode: 164, duration: 0.151s, episode steps: 14, steps per second: 93, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.135 [-0.778, 1.674], loss: 2.098775, mae: 4.545154, mean_q: 8.264597
 1790/5000: episode: 165, duration: 0.155s, episode steps: 16, steps per second: 103, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.090 [-0.810, 1.298], loss: 1.116850, mae: 4.370273, mean_q: 8.081714
 1799/5000: episode: 166, duration: 0.088s, episode steps: 9, steps per second: 102, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.128 [-1.223, 1.864], loss: 1.633186, mae: 4.582609, mean_q: 8.537574
 1809/5000: episode: 167, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.119 [-1.161, 1.798], loss: 1.055467, mae: 4.473445, mean_q: 8.416294
 1819/5000: episode: 168, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.326, 2.082], loss: 1.533171, mae: 4.641780, mean_q: 8.685934
 1833/5000: episode: 169, duration: 0.121s, episode steps: 14, steps per second: 116, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.076 [-1.177, 1.719], loss: 1.944857, mae: 4.756268, mean_q: 8.854489
 1845/5000: episode: 170, duration: 0.160s, episode steps: 12, steps per second: 75, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.957, 1.518], loss: 1.407320, mae: 4.613772, mean_q: 8.650897
 1857/5000: episode: 171, duration: 0.109s, episode steps: 12, steps per second: 110, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.099 [-1.015, 1.520], loss: 1.621332, mae: 4.721879, mean_q: 8.766198
 1876/5000: episode: 172, duration: 0.132s, episode steps: 19, steps per second: 144, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.123 [-0.744, 1.225], loss: 2.948927, mae: 4.769537, mean_q: 8.641991
 1895/5000: episode: 173, duration: 0.123s, episode steps: 19, steps per second: 155, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.072 [-0.735, 1.128], loss: 1.226862, mae: 4.686327, mean_q: 8.705743
 1904/5000: episode: 174, duration: 0.089s, episode steps: 9, steps per second: 102, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.104 [-1.032, 1.598], loss: 1.794063, mae: 4.941008, mean_q: 9.168735
 1913/5000: episode: 175, duration: 0.092s, episode steps: 9, steps per second: 98, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.136 [-1.150, 1.887], loss: 0.732051, mae: 4.778749, mean_q: 9.071996
 1925/5000: episode: 176, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.089 [-1.196, 1.810], loss: 2.267930, mae: 4.993860, mean_q: 9.299098
 1935/5000: episode: 177, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.142 [-1.545, 2.380], loss: 2.408178, mae: 5.148641, mean_q: 9.518293
 1943/5000: episode: 178, duration: 0.127s, episode steps: 8, steps per second: 63, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.581, 2.541], loss: 2.490978, mae: 5.097438, mean_q: 9.434625
 1951/5000: episode: 179, duration: 0.089s, episode steps: 8, steps per second: 90, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.570, 2.517], loss: 1.340217, mae: 4.997608, mean_q: 9.432140
 1962/5000: episode: 180, duration: 0.122s, episode steps: 11, steps per second: 91, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.119 [-1.546, 2.329], loss: 2.822553, mae: 5.187322, mean_q: 9.574150
 1973/5000: episode: 181, duration: 0.079s, episode steps: 11, steps per second: 139, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.133 [-1.539, 2.479], loss: 1.633390, mae: 5.033635, mean_q: 9.400720
 1983/5000: episode: 182, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.116 [-1.405, 2.161], loss: 2.034171, mae: 4.978302, mean_q: 9.308600
 1994/5000: episode: 183, duration: 0.083s, episode steps: 11, steps per second: 133, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.127 [-1.577, 2.338], loss: 2.842032, mae: 5.295521, mean_q: 9.775211
 2004/5000: episode: 184, duration: 0.077s, episode steps: 10, steps per second: 130, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.111 [-2.002, 2.982], loss: 1.786700, mae: 4.986466, mean_q: 9.248843
 2014/5000: episode: 185, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.158 [-1.553, 2.595], loss: 3.134647, mae: 5.232682, mean_q: 9.487054
 2023/5000: episode: 186, duration: 0.134s, episode steps: 9, steps per second: 67, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.375, 2.265], loss: 4.147699, mae: 5.308406, mean_q: 9.494036
 2036/5000: episode: 187, duration: 0.094s, episode steps: 13, steps per second: 139, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.080 [-1.162, 1.699], loss: 2.567080, mae: 5.154384, mean_q: 9.393833
 2046/5000: episode: 188, duration: 0.080s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.121 [-1.213, 1.812], loss: 1.914786, mae: 5.033726, mean_q: 9.198103
 2055/5000: episode: 189, duration: 0.103s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.152 [-1.194, 1.896], loss: 3.465645, mae: 5.466602, mean_q: 9.804726
 2065/5000: episode: 190, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.110 [-1.149, 1.781], loss: 2.316573, mae: 5.197017, mean_q: 9.478313
 2074/5000: episode: 191, duration: 0.067s, episode steps: 9, steps per second: 135, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.124 [-1.173, 1.844], loss: 2.188961, mae: 5.246216, mean_q: 9.604402
 2086/5000: episode: 192, duration: 0.084s, episode steps: 12, steps per second: 144, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-0.973, 1.643], loss: 2.053923, mae: 5.189360, mean_q: 9.497497
 2100/5000: episode: 193, duration: 0.112s, episode steps: 14, steps per second: 125, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.094 [-0.991, 1.429], loss: 2.002184, mae: 5.209592, mean_q: 9.634625
 2112/5000: episode: 194, duration: 0.109s, episode steps: 12, steps per second: 110, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.092 [-1.002, 1.510], loss: 1.528758, mae: 5.298075, mean_q: 9.909408
 2125/5000: episode: 195, duration: 0.120s, episode steps: 13, steps per second: 109, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.103 [-1.121, 1.688], loss: 2.217857, mae: 5.288889, mean_q: 9.840487
 2135/5000: episode: 196, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.129 [-0.937, 1.687], loss: 2.722369, mae: 5.327028, mean_q: 9.848267
 2145/5000: episode: 197, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.119 [-1.194, 1.769], loss: 3.239011, mae: 5.452727, mean_q: 9.905528
 2154/5000: episode: 198, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.131 [-1.204, 1.908], loss: 1.832875, mae: 5.430408, mean_q: 10.084481
 2163/5000: episode: 199, duration: 0.114s, episode steps: 9, steps per second: 79, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.161 [-1.345, 2.211], loss: 3.924464, mae: 5.435001, mean_q: 9.665277
 2173/5000: episode: 200, duration: 0.076s, episode steps: 10, steps per second: 132, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.124 [-1.398, 2.097], loss: 2.526243, mae: 5.260275, mean_q: 9.510973
 2182/5000: episode: 201, duration: 0.101s, episode steps: 9, steps per second: 89, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.129 [-0.951, 1.584], loss: 2.050274, mae: 5.166363, mean_q: 9.439543
 2193/5000: episode: 202, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.113 [-1.027, 1.529], loss: 2.198429, mae: 5.227308, mean_q: 9.506756
 2203/5000: episode: 203, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.132 [-1.180, 1.838], loss: 3.221857, mae: 5.255763, mean_q: 9.415183
 2212/5000: episode: 204, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.155 [-1.517, 2.423], loss: 2.317278, mae: 5.339962, mean_q: 9.689870
 2221/5000: episode: 205, duration: 0.164s, episode steps: 9, steps per second: 55, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.159 [-1.590, 2.549], loss: 2.589738, mae: 5.360859, mean_q: 9.735463
 2230/5000: episode: 206, duration: 0.099s, episode steps: 9, steps per second: 91, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.173 [-1.714, 2.822], loss: 2.231809, mae: 5.277069, mean_q: 9.728914
 2239/5000: episode: 207, duration: 0.109s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.782, 2.818], loss: 2.213346, mae: 5.264791, mean_q: 9.743282
 2249/5000: episode: 208, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.764, 2.719], loss: 2.240922, mae: 5.265371, mean_q: 9.737154
 2259/5000: episode: 209, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.521, 2.471], loss: 1.900578, mae: 5.271474, mean_q: 9.886621
 2270/5000: episode: 210, duration: 0.125s, episode steps: 11, steps per second: 88, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.423, 2.319], loss: 1.915384, mae: 5.373428, mean_q: 10.030233
 2281/5000: episode: 211, duration: 0.103s, episode steps: 11, steps per second: 107, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.116 [-1.797, 2.774], loss: 2.254610, mae: 5.417727, mean_q: 10.138198
 2290/5000: episode: 212, duration: 0.089s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.139 [-1.609, 2.537], loss: 2.453066, mae: 5.499150, mean_q: 10.286220
 2300/5000: episode: 213, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.155 [-1.533, 2.509], loss: 2.252331, mae: 5.429456, mean_q: 10.146227
 2309/5000: episode: 214, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.158 [-1.552, 2.498], loss: 1.587688, mae: 5.413756, mean_q: 10.196774
 2319/5000: episode: 215, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.150 [-1.536, 2.515], loss: 2.144324, mae: 5.364153, mean_q: 9.995013
 2328/5000: episode: 216, duration: 0.112s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.155 [-1.374, 2.304], loss: 2.073674, mae: 5.312142, mean_q: 9.895092
 2338/5000: episode: 217, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.587, 2.520], loss: 1.802261, mae: 5.370513, mean_q: 10.057920
 2350/5000: episode: 218, duration: 0.109s, episode steps: 12, steps per second: 110, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.087 [-1.605, 2.321], loss: 2.728330, mae: 5.463800, mean_q: 10.073064
 2360/5000: episode: 219, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.117 [-1.548, 2.416], loss: 2.346711, mae: 5.420657, mean_q: 10.028318
 2370/5000: episode: 220, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.116 [-1.167, 1.816], loss: 2.347601, mae: 5.397048, mean_q: 9.930646
 2380/5000: episode: 221, duration: 0.094s, episode steps: 10, steps per second: 107, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.126 [-1.213, 1.842], loss: 2.700611, mae: 5.455325, mean_q: 10.002542
 2390/5000: episode: 222, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.119 [-1.188, 1.871], loss: 2.522957, mae: 5.476307, mean_q: 10.092946
 2401/5000: episode: 223, duration: 0.113s, episode steps: 11, steps per second: 98, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.146 [-1.135, 1.941], loss: 1.690345, mae: 5.464000, mean_q: 10.239001
 2411/5000: episode: 224, duration: 0.074s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.114 [-1.200, 1.971], loss: 3.625621, mae: 5.497756, mean_q: 9.926282
 2423/5000: episode: 225, duration: 0.119s, episode steps: 12, steps per second: 101, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.119 [-1.317, 2.048], loss: 2.792851, mae: 5.569824, mean_q: 10.140499
 2431/5000: episode: 226, duration: 0.124s, episode steps: 8, steps per second: 64, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.154 [-1.372, 2.223], loss: 3.208208, mae: 5.459778, mean_q: 9.952246
 2442/5000: episode: 227, duration: 0.106s, episode steps: 11, steps per second: 104, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.106 [-1.337, 2.024], loss: 3.145340, mae: 5.628570, mean_q: 10.204945
 2454/5000: episode: 228, duration: 0.115s, episode steps: 12, steps per second: 105, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.098 [-1.382, 2.100], loss: 2.268792, mae: 5.467161, mean_q: 10.023299
 2464/5000: episode: 229, duration: 0.075s, episode steps: 10, steps per second: 134, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.542, 2.386], loss: 1.582552, mae: 5.298843, mean_q: 9.923663
 2475/5000: episode: 230, duration: 0.103s, episode steps: 11, steps per second: 106, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.102 [-1.406, 2.088], loss: 2.363913, mae: 5.659118, mean_q: 10.549244
 2491/5000: episode: 231, duration: 0.130s, episode steps: 16, steps per second: 123, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.062 [-1.415, 2.024], loss: 1.647240, mae: 5.368277, mean_q: 10.099436
 2500/5000: episode: 232, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.165 [-1.148, 1.974], loss: 1.521353, mae: 5.483139, mean_q: 10.341154
 2512/5000: episode: 233, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.092 [-1.405, 1.971], loss: 1.880966, mae: 5.488865, mean_q: 10.319151
 2525/5000: episode: 234, duration: 0.118s, episode steps: 13, steps per second: 110, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.116 [-1.188, 2.016], loss: 2.383224, mae: 5.506589, mean_q: 10.209751
 2536/5000: episode: 235, duration: 0.103s, episode steps: 11, steps per second: 107, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.135 [-1.349, 2.065], loss: 2.915498, mae: 5.605232, mean_q: 10.169783
 2545/5000: episode: 236, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.128 [-1.225, 1.918], loss: 2.056002, mae: 5.514679, mean_q: 10.095641
 2554/5000: episode: 237, duration: 0.093s, episode steps: 9, steps per second: 97, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.140 [-1.150, 1.922], loss: 2.207813, mae: 5.529660, mean_q: 10.134385
 2562/5000: episode: 238, duration: 0.086s, episode steps: 8, steps per second: 93, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.123 [-1.222, 1.986], loss: 2.218085, mae: 5.452917, mean_q: 10.027807
 2572/5000: episode: 239, duration: 0.135s, episode steps: 10, steps per second: 74, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.113 [-1.204, 1.778], loss: 2.064721, mae: 5.565082, mean_q: 10.281822
 2584/5000: episode: 240, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-1.166, 1.719], loss: 1.871727, mae: 5.490720, mean_q: 10.285415
 2596/5000: episode: 241, duration: 0.087s, episode steps: 12, steps per second: 138, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-1.179, 1.805], loss: 2.036294, mae: 5.378965, mean_q: 10.003556
 2612/5000: episode: 242, duration: 0.166s, episode steps: 16, steps per second: 97, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-1.219, 1.752], loss: 2.477270, mae: 5.411990, mean_q: 9.921753
 2623/5000: episode: 243, duration: 0.134s, episode steps: 11, steps per second: 82, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.138 [-1.130, 1.778], loss: 2.456415, mae: 5.459965, mean_q: 10.027348
 2635/5000: episode: 244, duration: 0.141s, episode steps: 12, steps per second: 85, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.085 [-1.018, 1.581], loss: 2.032711, mae: 5.379189, mean_q: 9.964340
 2644/5000: episode: 245, duration: 0.121s, episode steps: 9, steps per second: 74, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.119 [-1.226, 1.880], loss: 2.096265, mae: 5.252505, mean_q: 9.603083
 2654/5000: episode: 246, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.120 [-1.020, 1.593], loss: 1.849359, mae: 5.508210, mean_q: 10.238801
 2668/5000: episode: 247, duration: 0.147s, episode steps: 14, steps per second: 95, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.085 [-0.960, 1.623], loss: 2.484588, mae: 5.360476, mean_q: 9.857131
 2677/5000: episode: 248, duration: 0.091s, episode steps: 9, steps per second: 99, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.121 [-1.026, 1.710], loss: 2.288005, mae: 5.450181, mean_q: 9.988758
 2689/5000: episode: 249, duration: 0.173s, episode steps: 12, steps per second: 70, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.077 [-1.207, 1.703], loss: 1.477894, mae: 5.316569, mean_q: 9.911033
 2703/5000: episode: 250, duration: 0.157s, episode steps: 14, steps per second: 89, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.094 [-1.342, 1.978], loss: 2.218958, mae: 5.503138, mean_q: 10.224138
 2712/5000: episode: 251, duration: 0.089s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.160 [-1.167, 1.983], loss: 2.425834, mae: 5.328516, mean_q: 9.847147
 2723/5000: episode: 252, duration: 0.092s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.109 [-1.371, 2.041], loss: 1.895325, mae: 5.458683, mean_q: 10.130309
 2732/5000: episode: 253, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.154 [-1.130, 1.938], loss: 1.165347, mae: 5.336066, mean_q: 10.159307
 2741/5000: episode: 254, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.151 [-1.185, 1.986], loss: 1.343940, mae: 5.356678, mean_q: 10.140726
 2751/5000: episode: 255, duration: 0.072s, episode steps: 10, steps per second: 138, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.123 [-1.142, 1.871], loss: 1.707087, mae: 5.506842, mean_q: 10.368101
 2762/5000: episode: 256, duration: 0.085s, episode steps: 11, steps per second: 130, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.110 [-1.140, 1.775], loss: 2.223733, mae: 5.563603, mean_q: 10.313447
 2771/5000: episode: 257, duration: 0.085s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.144 [-0.946, 1.664], loss: 1.548369, mae: 5.343223, mean_q: 9.965520
 2782/5000: episode: 258, duration: 0.110s, episode steps: 11, steps per second: 100, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.116 [-0.961, 1.567], loss: 1.710220, mae: 5.307516, mean_q: 9.937575
 2795/5000: episode: 259, duration: 0.094s, episode steps: 13, steps per second: 138, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.095 [-0.988, 1.545], loss: 1.699836, mae: 5.370128, mean_q: 10.040150
 2813/5000: episode: 260, duration: 0.115s, episode steps: 18, steps per second: 157, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.992, 1.464], loss: 2.254529, mae: 5.352685, mean_q: 9.835051
 2823/5000: episode: 261, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.128 [-0.997, 1.630], loss: 1.890627, mae: 5.264632, mean_q: 9.762406
 2836/5000: episode: 262, duration: 0.095s, episode steps: 13, steps per second: 138, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.090 [-1.004, 1.512], loss: 2.156796, mae: 5.314934, mean_q: 9.737278
 2850/5000: episode: 263, duration: 0.136s, episode steps: 14, steps per second: 103, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.097 [-0.791, 1.301], loss: 1.414793, mae: 5.446918, mean_q: 10.204863
 2878/5000: episode: 264, duration: 0.306s, episode steps: 28, steps per second: 91, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.993, 1.382], loss: 2.104254, mae: 5.355439, mean_q: 9.900563
 2892/5000: episode: 265, duration: 0.095s, episode steps: 14, steps per second: 147, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.835, 1.299], loss: 1.843703, mae: 5.332697, mean_q: 9.847742
 2909/5000: episode: 266, duration: 0.115s, episode steps: 17, steps per second: 147, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.100 [-0.972, 1.447], loss: 1.995465, mae: 5.322895, mean_q: 9.875191
 2947/5000: episode: 267, duration: 0.258s, episode steps: 38, steps per second: 147, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.795, 1.103], loss: 1.573581, mae: 5.359415, mean_q: 9.987374
 2963/5000: episode: 268, duration: 0.148s, episode steps: 16, steps per second: 108, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.788, 1.180], loss: 2.109334, mae: 5.385089, mean_q: 9.925209
 2994/5000: episode: 269, duration: 0.218s, episode steps: 31, steps per second: 142, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.102 [-0.803, 1.180], loss: 1.736425, mae: 5.395387, mean_q: 10.020490
 3054/5000: episode: 270, duration: 0.399s, episode steps: 60, steps per second: 150, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.184 [-0.734, 2.283], loss: 1.998545, mae: 5.453781, mean_q: 10.054282
 3087/5000: episode: 271, duration: 0.226s, episode steps: 33, steps per second: 146, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.129 [-0.960, 0.579], loss: 1.697641, mae: 5.391362, mean_q: 9.999233
 3126/5000: episode: 272, duration: 0.288s, episode steps: 39, steps per second: 135, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.099 [-0.822, 0.605], loss: 1.435337, mae: 5.445566, mean_q: 10.208529
 3169/5000: episode: 273, duration: 0.294s, episode steps: 43, steps per second: 146, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.114 [-0.870, 0.548], loss: 1.693273, mae: 5.406854, mean_q: 10.047839
 3196/5000: episode: 274, duration: 0.274s, episode steps: 27, steps per second: 98, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.119 [-0.705, 0.365], loss: 2.136503, mae: 5.480899, mean_q: 10.162313
 3232/5000: episode: 275, duration: 0.301s, episode steps: 36, steps per second: 120, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.113 [-0.796, 0.744], loss: 1.487263, mae: 5.450082, mean_q: 10.172693
 3268/5000: episode: 276, duration: 0.291s, episode steps: 36, steps per second: 124, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.096 [-0.235, 0.705], loss: 1.620061, mae: 5.497997, mean_q: 10.253048
 3294/5000: episode: 277, duration: 0.256s, episode steps: 26, steps per second: 101, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.113 [-0.432, 0.748], loss: 1.494120, mae: 5.524955, mean_q: 10.337530
 3328/5000: episode: 278, duration: 0.284s, episode steps: 34, steps per second: 120, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.127 [-0.159, 0.607], loss: 1.505337, mae: 5.525793, mean_q: 10.320659
 3395/5000: episode: 279, duration: 0.544s, episode steps: 67, steps per second: 123, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.060 [-0.724, 0.705], loss: 1.818832, mae: 5.636702, mean_q: 10.467760
 3450/5000: episode: 280, duration: 0.384s, episode steps: 55, steps per second: 143, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.125 [-0.837, 0.479], loss: 1.422608, mae: 5.606716, mean_q: 10.472750
 3485/5000: episode: 281, duration: 0.255s, episode steps: 35, steps per second: 137, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.124 [-0.804, 0.248], loss: 1.607463, mae: 5.750816, mean_q: 10.769887
 3645/5000: episode: 282, duration: 1.100s, episode steps: 160, steps per second: 145, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.033 [-0.735, 0.531], loss: 1.780996, mae: 5.853346, mean_q: 10.922612
 3699/5000: episode: 283, duration: 0.324s, episode steps: 54, steps per second: 167, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.134 [-0.780, 0.342], loss: 1.817935, mae: 5.907372, mean_q: 10.976898
 3818/5000: episode: 284, duration: 0.769s, episode steps: 119, steps per second: 155, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.078 [-0.736, 0.534], loss: 1.854199, mae: 6.012404, mean_q: 11.216642
 3877/5000: episode: 285, duration: 0.411s, episode steps: 59, steps per second: 143, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.135 [-0.884, 0.492], loss: 1.812837, mae: 6.143807, mean_q: 11.498692
 3959/5000: episode: 286, duration: 0.672s, episode steps: 82, steps per second: 122, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.099 [-0.775, 0.482], loss: 1.775104, mae: 6.247323, mean_q: 11.721295
 4036/5000: episode: 287, duration: 0.685s, episode steps: 77, steps per second: 112, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.113 [-0.958, 0.543], loss: 1.986857, mae: 6.261961, mean_q: 11.665597
 4105/5000: episode: 288, duration: 0.549s, episode steps: 69, steps per second: 126, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.065 [-0.557, 0.641], loss: 1.629887, mae: 6.490444, mean_q: 12.227288
 4140/5000: episode: 289, duration: 0.309s, episode steps: 35, steps per second: 113, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.134 [-0.363, 0.847], loss: 2.371963, mae: 6.632807, mean_q: 12.366051
 4201/5000: episode: 290, duration: 0.464s, episode steps: 61, steps per second: 131, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.100 [-0.417, 0.677], loss: 1.988431, mae: 6.582776, mean_q: 12.301244
 4279/5000: episode: 291, duration: 0.563s, episode steps: 78, steps per second: 139, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.221 [-0.569, 1.497], loss: 2.294688, mae: 6.645879, mean_q: 12.356162
 4348/5000: episode: 292, duration: 0.476s, episode steps: 69, steps per second: 145, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.150 [-0.933, 0.267], loss: 2.159735, mae: 6.691597, mean_q: 12.496191
 4394/5000: episode: 293, duration: 0.297s, episode steps: 46, steps per second: 155, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.112 [-0.360, 0.790], loss: 1.958391, mae: 6.780542, mean_q: 12.699429
 4436/5000: episode: 294, duration: 0.350s, episode steps: 42, steps per second: 120, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.132 [-0.346, 0.792], loss: 2.098532, mae: 6.954820, mean_q: 13.007010
 4483/5000: episode: 295, duration: 0.348s, episode steps: 47, steps per second: 135, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.153 [-0.784, 0.205], loss: 2.057818, mae: 6.783680, mean_q: 12.666909
 4535/5000: episode: 296, duration: 0.374s, episode steps: 52, steps per second: 139, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.141 [-0.745, 0.190], loss: 2.136888, mae: 6.945175, mean_q: 13.013270
 4624/5000: episode: 297, duration: 0.623s, episode steps: 89, steps per second: 143, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.094 [-0.548, 0.833], loss: 2.087541, mae: 7.023202, mean_q: 13.179205
 4672/5000: episode: 298, duration: 0.392s, episode steps: 48, steps per second: 123, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.153 [-0.350, 0.957], loss: 2.370928, mae: 7.212307, mean_q: 13.517342
 4731/5000: episode: 299, duration: 0.448s, episode steps: 59, steps per second: 132, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.085 [-0.874, 0.424], loss: 1.988412, mae: 7.119732, mean_q: 13.399467
