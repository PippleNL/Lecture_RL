Training for 5000 steps ...
    9/5000: episode: 1, duration: 0.257s, episode steps: 9, steps per second: 35, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.882, 1.749], loss: --, mae: --, mean_q: --
   20/5000: episode: 2, duration: 1.068s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.088 [-2.715, 1.807], loss: 0.666121, mae: 0.983430, mean_q: 0.858823
   29/5000: episode: 3, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.111 [-2.226, 1.423], loss: 0.752602, mae: 0.999156, mean_q: 1.049906
   39/5000: episode: 4, duration: 0.146s, episode steps: 10, steps per second: 69, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.152 [-2.770, 1.738], loss: 0.693290, mae: 0.909143, mean_q: 1.116032
   47/5000: episode: 5, duration: 0.078s, episode steps: 8, steps per second: 103, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.563, 1.604], loss: 0.615300, mae: 0.867306, mean_q: 1.309369
   56/5000: episode: 6, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.762, 1.768], loss: 0.588502, mae: 0.786375, mean_q: 1.310416
   65/5000: episode: 7, duration: 0.131s, episode steps: 9, steps per second: 69, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-2.798, 1.804], loss: 0.527072, mae: 0.742717, mean_q: 1.479044
   74/5000: episode: 8, duration: 0.079s, episode steps: 9, steps per second: 114, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.817, 1.757], loss: 0.558465, mae: 0.713815, mean_q: 1.576242
   83/5000: episode: 9, duration: 0.112s, episode steps: 9, steps per second: 81, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-2.741, 1.805], loss: 0.445786, mae: 0.602188, mean_q: 1.601580
   94/5000: episode: 10, duration: 0.089s, episode steps: 11, steps per second: 124, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.124 [-2.719, 1.743], loss: 0.537840, mae: 0.623916, mean_q: 1.709895
  103/5000: episode: 11, duration: 0.104s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.802, 1.734], loss: 0.487390, mae: 0.584667, mean_q: 1.816234
  112/5000: episode: 12, duration: 0.109s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.795, 1.802], loss: 0.523026, mae: 0.562086, mean_q: 1.881625
  121/5000: episode: 13, duration: 0.120s, episode steps: 9, steps per second: 75, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.881, 1.804], loss: 0.626653, mae: 0.612447, mean_q: 1.976179
  130/5000: episode: 14, duration: 0.134s, episode steps: 9, steps per second: 67, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.812, 1.778], loss: 0.581982, mae: 0.556589, mean_q: 1.890781
  140/5000: episode: 15, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.069, 1.948], loss: 0.596136, mae: 0.549330, mean_q: 2.077211
  150/5000: episode: 16, duration: 0.089s, episode steps: 10, steps per second: 113, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.016, 1.962], loss: 0.561881, mae: 0.516567, mean_q: 2.065925
  159/5000: episode: 17, duration: 0.096s, episode steps: 9, steps per second: 94, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.873, 1.780], loss: 0.583428, mae: 0.523376, mean_q: 2.166001
  168/5000: episode: 18, duration: 0.086s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.835, 1.751], loss: 0.535376, mae: 0.527543, mean_q: 2.296146
  179/5000: episode: 19, duration: 0.089s, episode steps: 11, steps per second: 124, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.092 [-2.774, 1.785], loss: 0.546458, mae: 0.557841, mean_q: 2.309444
  189/5000: episode: 20, duration: 0.082s, episode steps: 10, steps per second: 122, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.091, 1.961], loss: 0.541852, mae: 0.585170, mean_q: 2.337801
  199/5000: episode: 21, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.105 [-2.979, 1.974], loss: 0.684390, mae: 0.682011, mean_q: 2.572290
  208/5000: episode: 22, duration: 0.071s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.743, 1.748], loss: 0.636470, mae: 0.675762, mean_q: 2.522586
  219/5000: episode: 23, duration: 0.084s, episode steps: 11, steps per second: 131, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.741, 1.796], loss: 0.660217, mae: 0.747120, mean_q: 2.583772
  229/5000: episode: 24, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.099, 1.970], loss: 0.680999, mae: 0.772417, mean_q: 2.708618
  238/5000: episode: 25, duration: 0.171s, episode steps: 9, steps per second: 53, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.861, 1.748], loss: 0.623609, mae: 0.813064, mean_q: 2.661525
  248/5000: episode: 26, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.984, 1.969], loss: 0.644210, mae: 0.820974, mean_q: 2.780467
  257/5000: episode: 27, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.175 [-2.889, 1.752], loss: 0.828173, mae: 0.912993, mean_q: 2.882059
  267/5000: episode: 28, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.026, 1.906], loss: 0.656349, mae: 0.894288, mean_q: 2.793516
  276/5000: episode: 29, duration: 0.116s, episode steps: 9, steps per second: 77, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-2.791, 1.807], loss: 0.677838, mae: 0.934464, mean_q: 2.804038
  285/5000: episode: 30, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.760, 1.804], loss: 0.626947, mae: 0.925362, mean_q: 2.828364
  293/5000: episode: 31, duration: 0.077s, episode steps: 8, steps per second: 103, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.506, 1.535], loss: 0.563728, mae: 0.930492, mean_q: 2.936023
  306/5000: episode: 32, duration: 0.103s, episode steps: 13, steps per second: 127, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.119 [-2.756, 1.716], loss: 0.545372, mae: 0.940824, mean_q: 3.034086
  316/5000: episode: 33, duration: 0.089s, episode steps: 10, steps per second: 112, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.551, 1.590], loss: 0.544051, mae: 0.960453, mean_q: 3.118681
  326/5000: episode: 34, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.067, 2.005], loss: 0.708920, mae: 1.084495, mean_q: 3.162893
  337/5000: episode: 35, duration: 0.096s, episode steps: 11, steps per second: 115, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.134 [-2.740, 1.737], loss: 0.590722, mae: 1.099168, mean_q: 3.106721
  346/5000: episode: 36, duration: 0.088s, episode steps: 9, steps per second: 102, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.825, 1.740], loss: 0.614396, mae: 1.148229, mean_q: 3.185163
  355/5000: episode: 37, duration: 0.100s, episode steps: 9, steps per second: 90, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.291, 1.390], loss: 0.529888, mae: 1.154230, mean_q: 3.265997
  365/5000: episode: 38, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.004, 1.973], loss: 0.600625, mae: 1.239910, mean_q: 3.396265
  373/5000: episode: 39, duration: 0.075s, episode steps: 8, steps per second: 107, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.490, 1.562], loss: 0.577098, mae: 1.283142, mean_q: 3.511560
  385/5000: episode: 40, duration: 0.112s, episode steps: 12, steps per second: 107, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.125 [-3.069, 1.942], loss: 0.610840, mae: 1.302229, mean_q: 3.567143
  396/5000: episode: 41, duration: 0.081s, episode steps: 11, steps per second: 135, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-3.302, 2.151], loss: 0.729743, mae: 1.421040, mean_q: 3.518910
  407/5000: episode: 42, duration: 0.081s, episode steps: 11, steps per second: 136, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.149 [-2.835, 1.714], loss: 0.664882, mae: 1.435680, mean_q: 3.438767
  417/5000: episode: 43, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.095 [-2.608, 1.794], loss: 0.636980, mae: 1.476369, mean_q: 3.514436
  426/5000: episode: 44, duration: 0.095s, episode steps: 9, steps per second: 95, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.830, 1.801], loss: 0.672985, mae: 1.535793, mean_q: 3.635815
  439/5000: episode: 45, duration: 0.119s, episode steps: 13, steps per second: 109, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.078 [-2.742, 1.800], loss: 0.583541, mae: 1.556885, mean_q: 3.689409
  449/5000: episode: 46, duration: 0.086s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.102 [-2.652, 1.793], loss: 0.638959, mae: 1.585688, mean_q: 3.778285
  459/5000: episode: 47, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.112, 1.995], loss: 0.559319, mae: 1.601107, mean_q: 3.793329
  469/5000: episode: 48, duration: 0.142s, episode steps: 10, steps per second: 70, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-3.107, 1.923], loss: 0.584127, mae: 1.648602, mean_q: 3.857602
  479/5000: episode: 49, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.103, 1.949], loss: 0.570704, mae: 1.686555, mean_q: 3.908577
  489/5000: episode: 50, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.152 [-2.256, 1.351], loss: 0.507000, mae: 1.683836, mean_q: 4.000966
  498/5000: episode: 51, duration: 0.090s, episode steps: 9, steps per second: 100, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.133 [-2.216, 1.406], loss: 0.540580, mae: 1.714331, mean_q: 4.075992
  506/5000: episode: 52, duration: 0.071s, episode steps: 8, steps per second: 113, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.537, 1.534], loss: 0.626744, mae: 1.764747, mean_q: 4.064243
  514/5000: episode: 53, duration: 0.099s, episode steps: 8, steps per second: 81, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.526, 1.526], loss: 0.555947, mae: 1.736133, mean_q: 4.036404
  524/5000: episode: 54, duration: 0.099s, episode steps: 10, steps per second: 101, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-3.135, 1.952], loss: 0.585853, mae: 1.779129, mean_q: 4.060281
  534/5000: episode: 55, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.083, 1.975], loss: 0.449879, mae: 1.742607, mean_q: 4.178425
  543/5000: episode: 56, duration: 0.092s, episode steps: 9, steps per second: 97, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.154 [-2.157, 1.340], loss: 0.617371, mae: 1.799747, mean_q: 4.281045
  553/5000: episode: 57, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.141 [-2.513, 1.553], loss: 0.513794, mae: 1.754236, mean_q: 4.239067
  564/5000: episode: 58, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.112 [-2.774, 1.790], loss: 0.510228, mae: 1.777020, mean_q: 4.262384
  574/5000: episode: 59, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.966, 1.915], loss: 0.451416, mae: 1.776214, mean_q: 4.419363
  583/5000: episode: 60, duration: 0.069s, episode steps: 9, steps per second: 131, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.807, 1.720], loss: 0.520639, mae: 1.848758, mean_q: 4.466479
  595/5000: episode: 61, duration: 0.139s, episode steps: 12, steps per second: 87, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.128 [-2.579, 1.556], loss: 0.456437, mae: 1.852858, mean_q: 4.480687
  604/5000: episode: 62, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.818, 1.767], loss: 0.531395, mae: 1.890296, mean_q: 4.475697
  613/5000: episode: 63, duration: 0.108s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.358, 1.416], loss: 0.457382, mae: 1.891764, mean_q: 4.537130
  622/5000: episode: 64, duration: 0.095s, episode steps: 9, steps per second: 95, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-2.831, 1.804], loss: 0.445315, mae: 1.897694, mean_q: 4.593300
  630/5000: episode: 65, duration: 0.080s, episode steps: 8, steps per second: 100, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.523, 1.552], loss: 0.434117, mae: 1.934669, mean_q: 4.592728
  640/5000: episode: 66, duration: 0.094s, episode steps: 10, steps per second: 106, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.138 [-2.584, 1.586], loss: 0.532827, mae: 2.012968, mean_q: 4.588312
  648/5000: episode: 67, duration: 0.075s, episode steps: 8, steps per second: 106, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-2.524, 1.562], loss: 0.450495, mae: 2.017018, mean_q: 4.669729
  658/5000: episode: 68, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.709, 1.780], loss: 0.436308, mae: 2.012764, mean_q: 4.775436
  667/5000: episode: 69, duration: 0.070s, episode steps: 9, steps per second: 129, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.251, 1.386], loss: 0.390837, mae: 2.021696, mean_q: 4.762611
  679/5000: episode: 70, duration: 0.094s, episode steps: 12, steps per second: 127, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.097 [-2.568, 1.584], loss: 0.364633, mae: 2.054482, mean_q: 4.848378
  690/5000: episode: 71, duration: 0.081s, episode steps: 11, steps per second: 135, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.127 [-2.346, 1.517], loss: 0.473658, mae: 2.125201, mean_q: 4.806417
  700/5000: episode: 72, duration: 0.159s, episode steps: 10, steps per second: 63, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.112 [-2.629, 1.787], loss: 0.436968, mae: 2.143851, mean_q: 4.761303
  709/5000: episode: 73, duration: 0.085s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.444, 1.521], loss: 0.338985, mae: 2.127866, mean_q: 4.942617
  720/5000: episode: 74, duration: 0.104s, episode steps: 11, steps per second: 106, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.106 [-2.322, 1.596], loss: 0.447624, mae: 2.185910, mean_q: 4.907907
  730/5000: episode: 75, duration: 0.082s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.131 [-2.425, 1.585], loss: 0.395589, mae: 2.176361, mean_q: 4.945463
  743/5000: episode: 76, duration: 0.094s, episode steps: 13, steps per second: 138, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.063 [-2.410, 1.607], loss: 0.368530, mae: 2.180358, mean_q: 4.933246
  751/5000: episode: 77, duration: 0.065s, episode steps: 8, steps per second: 123, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.143 [-2.223, 1.395], loss: 0.398956, mae: 2.202529, mean_q: 4.977649
  762/5000: episode: 78, duration: 0.090s, episode steps: 11, steps per second: 123, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.109 [-2.353, 1.603], loss: 0.347236, mae: 2.198246, mean_q: 5.136803
  773/5000: episode: 79, duration: 0.144s, episode steps: 11, steps per second: 77, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.116 [-2.466, 1.600], loss: 0.392571, mae: 2.212957, mean_q: 5.089303
  783/5000: episode: 80, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.129 [-2.199, 1.414], loss: 0.293761, mae: 2.202272, mean_q: 5.099196
  792/5000: episode: 81, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.122 [-2.410, 1.577], loss: 0.342655, mae: 2.244603, mean_q: 5.293081
  802/5000: episode: 82, duration: 0.148s, episode steps: 10, steps per second: 67, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.123 [-2.006, 1.222], loss: 0.348067, mae: 2.264679, mean_q: 5.161733
  811/5000: episode: 83, duration: 0.082s, episode steps: 9, steps per second: 110, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.132 [-2.216, 1.349], loss: 0.269847, mae: 2.278794, mean_q: 5.392399
  820/5000: episode: 84, duration: 0.072s, episode steps: 9, steps per second: 126, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.457, 1.609], loss: 0.281411, mae: 2.311610, mean_q: 5.300238
  832/5000: episode: 85, duration: 0.090s, episode steps: 12, steps per second: 133, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.102 [-2.164, 1.360], loss: 0.308289, mae: 2.373948, mean_q: 5.438359
  842/5000: episode: 86, duration: 0.091s, episode steps: 10, steps per second: 110, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.131 [-2.126, 1.378], loss: 0.350894, mae: 2.416129, mean_q: 5.303628
  851/5000: episode: 87, duration: 0.102s, episode steps: 9, steps per second: 89, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.131 [-2.173, 1.417], loss: 0.221971, mae: 2.370352, mean_q: 5.357350
  862/5000: episode: 88, duration: 0.115s, episode steps: 11, steps per second: 96, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.125 [-1.987, 1.145], loss: 0.323356, mae: 2.398854, mean_q: 5.367486
  871/5000: episode: 89, duration: 0.103s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.126 [-2.131, 1.344], loss: 0.348785, mae: 2.431445, mean_q: 5.543075
  880/5000: episode: 90, duration: 0.094s, episode steps: 9, steps per second: 95, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.142 [-2.196, 1.411], loss: 0.263870, mae: 2.394747, mean_q: 5.475572
  894/5000: episode: 91, duration: 0.116s, episode steps: 14, steps per second: 121, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.097 [-2.127, 1.233], loss: 0.261981, mae: 2.445111, mean_q: 5.680611
  903/5000: episode: 92, duration: 0.081s, episode steps: 9, steps per second: 111, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.136 [-2.160, 1.347], loss: 0.273965, mae: 2.490499, mean_q: 5.756522
  913/5000: episode: 93, duration: 0.087s, episode steps: 10, steps per second: 116, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.134 [-2.242, 1.370], loss: 0.320612, mae: 2.518619, mean_q: 5.621425
  922/5000: episode: 94, duration: 0.083s, episode steps: 9, steps per second: 109, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.136 [-2.125, 1.347], loss: 0.243826, mae: 2.517575, mean_q: 5.617994
  933/5000: episode: 95, duration: 0.092s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.091 [-2.264, 1.596], loss: 0.323193, mae: 2.557825, mean_q: 5.600388
  943/5000: episode: 96, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.114 [-2.201, 1.417], loss: 0.299771, mae: 2.584023, mean_q: 5.689275
  951/5000: episode: 97, duration: 0.077s, episode steps: 8, steps per second: 104, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.142 [-2.168, 1.356], loss: 0.298798, mae: 2.648689, mean_q: 5.907977
  962/5000: episode: 98, duration: 0.089s, episode steps: 11, steps per second: 123, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.108 [-2.044, 1.395], loss: 0.341338, mae: 2.651451, mean_q: 5.711686
  972/5000: episode: 99, duration: 0.104s, episode steps: 10, steps per second: 96, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.122 [-2.116, 1.359], loss: 0.322205, mae: 2.652459, mean_q: 5.709266
  981/5000: episode: 100, duration: 0.077s, episode steps: 9, steps per second: 118, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.172 [-2.311, 1.333], loss: 0.272876, mae: 2.662670, mean_q: 5.807859
  990/5000: episode: 101, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.151 [-2.190, 1.378], loss: 0.376891, mae: 2.673434, mean_q: 5.641785
 1003/5000: episode: 102, duration: 0.160s, episode steps: 13, steps per second: 81, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.117 [-2.040, 1.170], loss: 0.260181, mae: 2.652396, mean_q: 5.735910
 1014/5000: episode: 103, duration: 0.091s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.109 [-2.317, 1.383], loss: 0.290282, mae: 2.705287, mean_q: 5.834898
 1024/5000: episode: 104, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.437, 1.566], loss: 0.271781, mae: 2.708498, mean_q: 5.886905
 1033/5000: episode: 105, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.149 [-2.203, 1.354], loss: 0.308319, mae: 2.712273, mean_q: 5.821676
 1042/5000: episode: 106, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.145 [-2.168, 1.363], loss: 0.272415, mae: 2.746259, mean_q: 6.009859
 1051/5000: episode: 107, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.116 [-2.121, 1.384], loss: 0.296119, mae: 2.696482, mean_q: 5.765140
 1061/5000: episode: 108, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.089 [-1.850, 1.218], loss: 0.261880, mae: 2.750329, mean_q: 5.995101
 1072/5000: episode: 109, duration: 0.084s, episode steps: 11, steps per second: 131, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.104 [-2.277, 1.594], loss: 0.279480, mae: 2.774107, mean_q: 6.004393
 1083/5000: episode: 110, duration: 0.125s, episode steps: 11, steps per second: 88, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.134 [-2.162, 1.372], loss: 0.217431, mae: 2.744915, mean_q: 5.938531
 1094/5000: episode: 111, duration: 0.087s, episode steps: 11, steps per second: 127, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.114 [-2.118, 1.357], loss: 0.277928, mae: 2.821867, mean_q: 6.026893
 1103/5000: episode: 112, duration: 0.094s, episode steps: 9, steps per second: 95, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.131 [-1.939, 1.196], loss: 0.272414, mae: 2.838826, mean_q: 6.006993
 1113/5000: episode: 113, duration: 0.093s, episode steps: 10, steps per second: 108, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.102 [-2.100, 1.394], loss: 0.285284, mae: 2.898625, mean_q: 6.142295
 1124/5000: episode: 114, duration: 0.083s, episode steps: 11, steps per second: 132, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.133 [-2.078, 1.333], loss: 0.242683, mae: 2.797292, mean_q: 5.900946
 1135/5000: episode: 115, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.137 [-2.124, 1.333], loss: 0.248693, mae: 2.886316, mean_q: 6.159565
 1146/5000: episode: 116, duration: 0.087s, episode steps: 11, steps per second: 126, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.114 [-2.085, 1.339], loss: 0.230725, mae: 2.846185, mean_q: 6.022829
 1155/5000: episode: 117, duration: 0.083s, episode steps: 9, steps per second: 109, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.139 [-1.913, 1.184], loss: 0.281666, mae: 2.894372, mean_q: 6.029063
 1164/5000: episode: 118, duration: 0.156s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.146 [-1.941, 1.152], loss: 0.219571, mae: 2.802640, mean_q: 5.851910
 1172/5000: episode: 119, duration: 0.067s, episode steps: 8, steps per second: 119, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.154 [-2.234, 1.382], loss: 0.216605, mae: 2.930827, mean_q: 6.218426
 1180/5000: episode: 120, duration: 0.071s, episode steps: 8, steps per second: 112, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.130 [-1.896, 1.219], loss: 0.322113, mae: 2.880330, mean_q: 5.928347
 1189/5000: episode: 121, duration: 0.093s, episode steps: 9, steps per second: 97, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.157 [-2.173, 1.334], loss: 0.221947, mae: 2.881462, mean_q: 6.064767
 1201/5000: episode: 122, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.099 [-2.082, 1.354], loss: 0.226667, mae: 2.941550, mean_q: 6.138991
 1209/5000: episode: 123, duration: 0.063s, episode steps: 8, steps per second: 126, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.157 [-2.188, 1.332], loss: 0.152611, mae: 2.979730, mean_q: 6.291635
 1220/5000: episode: 124, duration: 0.089s, episode steps: 11, steps per second: 124, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.116 [-2.297, 1.561], loss: 0.198369, mae: 2.940639, mean_q: 6.121916
 1230/5000: episode: 125, duration: 0.080s, episode steps: 10, steps per second: 124, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.128 [-2.115, 1.364], loss: 0.242111, mae: 2.981503, mean_q: 6.234841
 1238/5000: episode: 126, duration: 0.118s, episode steps: 8, steps per second: 68, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.142 [-1.912, 1.204], loss: 0.258124, mae: 2.951337, mean_q: 6.091819
 1247/5000: episode: 127, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.120 [-1.648, 1.016], loss: 0.226290, mae: 2.875099, mean_q: 5.926469
 1258/5000: episode: 128, duration: 0.103s, episode steps: 11, steps per second: 107, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.102 [-2.038, 1.412], loss: 0.200560, mae: 2.944875, mean_q: 6.084070
 1270/5000: episode: 129, duration: 0.176s, episode steps: 12, steps per second: 68, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.115 [-2.030, 1.347], loss: 0.177362, mae: 3.106680, mean_q: 6.435252
 1282/5000: episode: 130, duration: 0.151s, episode steps: 12, steps per second: 79, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.099 [-1.769, 1.150], loss: 0.212593, mae: 3.048124, mean_q: 6.178113
 1292/5000: episode: 131, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.125 [-1.793, 1.154], loss: 0.171043, mae: 3.138486, mean_q: 6.358959
 1305/5000: episode: 132, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.078 [-1.462, 0.991], loss: 0.230414, mae: 3.137136, mean_q: 6.280850
 1317/5000: episode: 133, duration: 0.086s, episode steps: 12, steps per second: 139, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.094 [-1.756, 1.201], loss: 0.274392, mae: 3.152196, mean_q: 6.243868
 1327/5000: episode: 134, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.116 [-1.808, 1.195], loss: 0.223044, mae: 3.132338, mean_q: 6.221639
 1337/5000: episode: 135, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.127 [-1.575, 0.964], loss: 0.295922, mae: 3.166951, mean_q: 6.213876
 1350/5000: episode: 136, duration: 0.126s, episode steps: 13, steps per second: 103, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.116 [-1.768, 1.130], loss: 0.211579, mae: 3.214871, mean_q: 6.376653
 1362/5000: episode: 137, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.117 [-1.543, 0.981], loss: 0.219376, mae: 3.267775, mean_q: 6.474448
 1375/5000: episode: 138, duration: 0.110s, episode steps: 13, steps per second: 118, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.090 [-1.508, 1.000], loss: 0.277535, mae: 3.206631, mean_q: 6.277397
 1391/5000: episode: 139, duration: 0.114s, episode steps: 16, steps per second: 140, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.076 [-1.728, 1.126], loss: 0.174408, mae: 3.184205, mean_q: 6.305326
 1400/5000: episode: 140, duration: 0.104s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.124 [-1.512, 0.981], loss: 0.301818, mae: 3.290165, mean_q: 6.403887
 1414/5000: episode: 141, duration: 0.139s, episode steps: 14, steps per second: 101, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.103 [-1.486, 0.956], loss: 0.218188, mae: 3.288975, mean_q: 6.448435
 1427/5000: episode: 142, duration: 0.110s, episode steps: 13, steps per second: 119, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.111 [-1.118, 0.582], loss: 0.234142, mae: 3.282007, mean_q: 6.380928
 1447/5000: episode: 143, duration: 0.144s, episode steps: 20, steps per second: 139, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.043 [-1.367, 1.018], loss: 0.249514, mae: 3.376200, mean_q: 6.548541
 1464/5000: episode: 144, duration: 0.116s, episode steps: 17, steps per second: 146, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.096 [-1.437, 0.953], loss: 0.273626, mae: 3.350001, mean_q: 6.460430
 1525/5000: episode: 145, duration: 0.403s, episode steps: 61, steps per second: 151, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.119 [-1.508, 1.654], loss: 0.265109, mae: 3.509944, mean_q: 6.732851
 1545/5000: episode: 146, duration: 0.134s, episode steps: 20, steps per second: 150, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.004 [-2.349, 3.251], loss: 0.367230, mae: 3.472324, mean_q: 6.570485
 1565/5000: episode: 147, duration: 0.208s, episode steps: 20, steps per second: 96, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.029 [-2.368, 3.402], loss: 0.219077, mae: 3.661558, mean_q: 7.033582
 1574/5000: episode: 148, duration: 0.117s, episode steps: 9, steps per second: 77, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.760, 2.839], loss: 0.288255, mae: 3.547009, mean_q: 6.757116
 1584/5000: episode: 149, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.948, 2.980], loss: 1.237815, mae: 3.800032, mean_q: 7.198733
 1594/5000: episode: 150, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.111 [-1.576, 2.483], loss: 0.783718, mae: 3.730843, mean_q: 7.111922
 1603/5000: episode: 151, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.758, 2.764], loss: 0.656779, mae: 3.791541, mean_q: 7.239769
 1674/5000: episode: 152, duration: 0.425s, episode steps: 71, steps per second: 167, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.108 [-1.586, 1.869], loss: 0.659525, mae: 3.898362, mean_q: 7.392361
 1686/5000: episode: 153, duration: 0.086s, episode steps: 12, steps per second: 139, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.121 [-1.557, 2.448], loss: 0.578784, mae: 3.884152, mean_q: 7.388565
 1696/5000: episode: 154, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.121 [-1.392, 2.087], loss: 1.152289, mae: 4.073786, mean_q: 7.692740
 1712/5000: episode: 155, duration: 0.119s, episode steps: 16, steps per second: 135, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.582, 1.059], loss: 0.957869, mae: 4.102408, mean_q: 7.797746
 1724/5000: episode: 156, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.102 [-1.191, 1.799], loss: 1.493833, mae: 4.182993, mean_q: 7.848027
 1746/5000: episode: 157, duration: 0.250s, episode steps: 22, steps per second: 88, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.115 [-0.609, 0.970], loss: 0.814689, mae: 4.205135, mean_q: 7.973465
 1769/5000: episode: 158, duration: 0.149s, episode steps: 23, steps per second: 154, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.073 [-0.402, 1.028], loss: 1.320370, mae: 4.293703, mean_q: 8.108274
 1779/5000: episode: 159, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-0.763, 1.490], loss: 0.352767, mae: 4.170396, mean_q: 7.963830
 1794/5000: episode: 160, duration: 0.167s, episode steps: 15, steps per second: 90, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.101 [-0.925, 1.484], loss: 0.418025, mae: 4.375843, mean_q: 8.397333
 1803/5000: episode: 161, duration: 0.087s, episode steps: 9, steps per second: 103, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.147 [-0.992, 1.666], loss: 1.516650, mae: 4.500557, mean_q: 8.519059
 1812/5000: episode: 162, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.151 [-0.961, 1.648], loss: 1.778839, mae: 4.416293, mean_q: 8.278045
 1824/5000: episode: 163, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.113 [-1.409, 2.181], loss: 0.746034, mae: 4.371348, mean_q: 8.250599
 1834/5000: episode: 164, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.156 [-1.547, 2.613], loss: 1.168087, mae: 4.420765, mean_q: 8.334361
 1843/5000: episode: 165, duration: 0.070s, episode steps: 9, steps per second: 128, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.786, 2.783], loss: 2.251302, mae: 4.578253, mean_q: 8.544443
 1852/5000: episode: 166, duration: 0.159s, episode steps: 9, steps per second: 57, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.812, 2.879], loss: 2.352679, mae: 4.595086, mean_q: 8.529261
 1863/5000: episode: 167, duration: 0.087s, episode steps: 11, steps per second: 127, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.115 [-1.362, 2.275], loss: 1.641204, mae: 4.613788, mean_q: 8.627988
 1873/5000: episode: 168, duration: 0.077s, episode steps: 10, steps per second: 130, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.914, 3.093], loss: 1.359903, mae: 4.781415, mean_q: 9.041380
 1883/5000: episode: 169, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.118 [-1.765, 2.669], loss: 1.612193, mae: 4.744680, mean_q: 8.865673
 1893/5000: episode: 170, duration: 0.133s, episode steps: 10, steps per second: 75, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.143 [-1.547, 2.527], loss: 1.309612, mae: 4.743964, mean_q: 8.964857
 1902/5000: episode: 171, duration: 0.132s, episode steps: 9, steps per second: 68, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.797, 2.772], loss: 2.127606, mae: 4.812324, mean_q: 9.067973
 1911/5000: episode: 172, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.724, 2.832], loss: 1.928880, mae: 4.677370, mean_q: 8.834897
 1923/5000: episode: 173, duration: 0.090s, episode steps: 12, steps per second: 134, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.120 [-1.907, 2.974], loss: 1.699333, mae: 4.739357, mean_q: 8.847541
 1932/5000: episode: 174, duration: 0.120s, episode steps: 9, steps per second: 75, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.113 [-1.425, 2.272], loss: 2.939169, mae: 4.938562, mean_q: 9.137155
 1940/5000: episode: 175, duration: 0.064s, episode steps: 8, steps per second: 125, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.543, 2.560], loss: 2.115055, mae: 4.719856, mean_q: 8.773356
 1950/5000: episode: 176, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.113 [-1.975, 2.987], loss: 3.065264, mae: 5.071054, mean_q: 9.274080
 1960/5000: episode: 177, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.937, 3.054], loss: 1.228063, mae: 4.787814, mean_q: 8.949450
 1968/5000: episode: 178, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.581, 2.541], loss: 2.915439, mae: 4.855757, mean_q: 8.797680
 1976/5000: episode: 179, duration: 0.072s, episode steps: 8, steps per second: 111, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.137 [-1.374, 2.178], loss: 1.996726, mae: 4.768515, mean_q: 8.768690
 1988/5000: episode: 180, duration: 0.102s, episode steps: 12, steps per second: 118, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-1.350, 1.994], loss: 0.948698, mae: 4.637936, mean_q: 8.610696
 1998/5000: episode: 181, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.140 [-1.540, 2.519], loss: 1.690839, mae: 4.657998, mean_q: 8.661627
 2008/5000: episode: 182, duration: 0.077s, episode steps: 10, steps per second: 130, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.130 [-1.795, 2.741], loss: 2.383969, mae: 4.873818, mean_q: 9.071012
 2020/5000: episode: 183, duration: 0.096s, episode steps: 12, steps per second: 125, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.122 [-1.970, 3.041], loss: 1.903169, mae: 4.959831, mean_q: 9.290436
 2030/5000: episode: 184, duration: 0.134s, episode steps: 10, steps per second: 75, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.106 [-1.806, 2.643], loss: 1.836919, mae: 5.041970, mean_q: 9.489194
 2039/5000: episode: 185, duration: 0.081s, episode steps: 9, steps per second: 111, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [-1.746, 2.839], loss: 2.864861, mae: 5.093637, mean_q: 9.428223
 2048/5000: episode: 186, duration: 0.071s, episode steps: 9, steps per second: 126, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.764, 2.841], loss: 2.030735, mae: 5.077932, mean_q: 9.532386
 2058/5000: episode: 187, duration: 0.077s, episode steps: 10, steps per second: 129, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.114 [-1.749, 2.632], loss: 2.469469, mae: 5.210999, mean_q: 9.644624
 2067/5000: episode: 188, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.801, 2.809], loss: 2.052278, mae: 5.200825, mean_q: 9.685817
 2075/5000: episode: 189, duration: 0.066s, episode steps: 8, steps per second: 120, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.587, 2.568], loss: 3.934980, mae: 5.118145, mean_q: 9.340475
 2085/5000: episode: 190, duration: 0.106s, episode steps: 10, steps per second: 94, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.116 [-1.152, 1.850], loss: 3.083407, mae: 5.142191, mean_q: 9.387856
 2094/5000: episode: 191, duration: 0.075s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.135 [-1.176, 1.926], loss: 2.965035, mae: 5.503653, mean_q: 10.045421
 2104/5000: episode: 192, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.124 [-1.359, 2.132], loss: 3.416727, mae: 5.116212, mean_q: 9.258226
 2116/5000: episode: 193, duration: 0.173s, episode steps: 12, steps per second: 69, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-1.187, 1.741], loss: 2.366940, mae: 5.097064, mean_q: 9.376008
 2126/5000: episode: 194, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.116 [-1.394, 2.178], loss: 3.072788, mae: 5.142038, mean_q: 9.358118
 2138/5000: episode: 195, duration: 0.087s, episode steps: 12, steps per second: 138, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.122 [-1.905, 3.004], loss: 2.649979, mae: 5.277317, mean_q: 9.682872
 2148/5000: episode: 196, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.148 [-1.135, 2.068], loss: 2.666435, mae: 5.288407, mean_q: 9.820801
 2157/5000: episode: 197, duration: 0.072s, episode steps: 9, steps per second: 124, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.783, 2.764], loss: 2.111971, mae: 5.295106, mean_q: 9.861691
 2166/5000: episode: 198, duration: 0.093s, episode steps: 9, steps per second: 97, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.147 [-1.593, 2.489], loss: 2.146857, mae: 5.229738, mean_q: 9.767462
 2175/5000: episode: 199, duration: 0.075s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.172 [-1.542, 2.549], loss: 1.481684, mae: 5.153179, mean_q: 9.555939
 2185/5000: episode: 200, duration: 0.095s, episode steps: 10, steps per second: 106, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.148 [-1.790, 2.756], loss: 3.549042, mae: 5.520631, mean_q: 10.003099
 2193/5000: episode: 201, duration: 0.063s, episode steps: 8, steps per second: 127, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.537, 2.505], loss: 1.106185, mae: 5.116421, mean_q: 9.630252
 2202/5000: episode: 202, duration: 0.077s, episode steps: 9, steps per second: 117, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.809, 2.788], loss: 2.292207, mae: 5.226944, mean_q: 9.702288
 2211/5000: episode: 203, duration: 0.095s, episode steps: 9, steps per second: 95, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.148 [-1.572, 2.495], loss: 3.295667, mae: 5.487943, mean_q: 10.052257
 2221/5000: episode: 204, duration: 0.082s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.159 [-1.324, 2.184], loss: 2.393940, mae: 5.389387, mean_q: 9.932717
 2230/5000: episode: 205, duration: 0.082s, episode steps: 9, steps per second: 109, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.143 [-1.201, 1.969], loss: 3.524348, mae: 5.544696, mean_q: 10.040385
 2239/5000: episode: 206, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.163 [-1.325, 2.246], loss: 2.078144, mae: 5.215163, mean_q: 9.648820
 2248/5000: episode: 207, duration: 0.089s, episode steps: 9, steps per second: 101, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.142 [-1.393, 2.240], loss: 3.264749, mae: 5.464277, mean_q: 9.880112
 2258/5000: episode: 208, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.130 [-1.375, 2.138], loss: 2.652697, mae: 5.472449, mean_q: 9.975502
 2270/5000: episode: 209, duration: 0.087s, episode steps: 12, steps per second: 138, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.136 [-1.327, 2.181], loss: 1.969983, mae: 5.359098, mean_q: 9.907882
 2282/5000: episode: 210, duration: 0.087s, episode steps: 12, steps per second: 137, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.100 [-1.617, 2.580], loss: 3.048729, mae: 5.480289, mean_q: 10.069305
 2292/5000: episode: 211, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.600, 2.398], loss: 2.606773, mae: 5.248321, mean_q: 9.680048
 2301/5000: episode: 212, duration: 0.093s, episode steps: 9, steps per second: 97, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.416, 2.309], loss: 2.443186, mae: 5.429953, mean_q: 9.992621
 2311/5000: episode: 213, duration: 0.079s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.160 [-1.726, 2.741], loss: 1.868705, mae: 5.410258, mean_q: 10.125357
 2320/5000: episode: 214, duration: 0.116s, episode steps: 9, steps per second: 77, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.748, 2.843], loss: 2.037741, mae: 5.444666, mean_q: 10.131577
 2330/5000: episode: 215, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.536, 2.510], loss: 1.357181, mae: 5.317358, mean_q: 10.035008
 2340/5000: episode: 216, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.153 [-1.570, 2.629], loss: 2.056042, mae: 5.407241, mean_q: 10.158536
 2349/5000: episode: 217, duration: 0.080s, episode steps: 9, steps per second: 112, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.781, 2.769], loss: 2.450712, mae: 5.634359, mean_q: 10.545687
 2361/5000: episode: 218, duration: 0.171s, episode steps: 12, steps per second: 70, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.100 [-1.609, 2.449], loss: 2.336240, mae: 5.620908, mean_q: 10.510999
 2371/5000: episode: 219, duration: 0.142s, episode steps: 10, steps per second: 71, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.127 [-1.741, 2.659], loss: 2.134413, mae: 5.537650, mean_q: 10.381287
 2381/5000: episode: 220, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.146 [-1.753, 2.735], loss: 1.652721, mae: 5.393343, mean_q: 10.129574
 2390/5000: episode: 221, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.605, 2.502], loss: 1.884866, mae: 5.507360, mean_q: 10.379214
 2400/5000: episode: 222, duration: 0.078s, episode steps: 10, steps per second: 129, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.129 [-1.385, 2.211], loss: 1.886292, mae: 5.428505, mean_q: 10.238305
 2409/5000: episode: 223, duration: 0.074s, episode steps: 9, steps per second: 122, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.718, 2.771], loss: 1.620930, mae: 5.455904, mean_q: 10.366716
 2418/5000: episode: 224, duration: 0.073s, episode steps: 9, steps per second: 123, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.121 [-1.586, 2.444], loss: 1.660746, mae: 5.469264, mean_q: 10.417439
 2432/5000: episode: 225, duration: 0.102s, episode steps: 14, steps per second: 137, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.104 [-1.710, 2.712], loss: 2.375889, mae: 5.526568, mean_q: 10.394990
 2440/5000: episode: 226, duration: 0.068s, episode steps: 8, steps per second: 118, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.154 [-1.372, 2.223], loss: 0.903180, mae: 5.481197, mean_q: 10.532054
 2452/5000: episode: 227, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.107 [-1.728, 2.639], loss: 1.960419, mae: 5.656009, mean_q: 10.654590
 2462/5000: episode: 228, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.773, 2.713], loss: 2.629445, mae: 5.665139, mean_q: 10.529315
 2472/5000: episode: 229, duration: 0.081s, episode steps: 10, steps per second: 124, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.738, 2.716], loss: 2.102801, mae: 5.469882, mean_q: 10.290496
 2487/5000: episode: 230, duration: 0.160s, episode steps: 15, steps per second: 94, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.068 [-1.411, 2.222], loss: 2.371796, mae: 5.611589, mean_q: 10.469349
 2502/5000: episode: 231, duration: 0.105s, episode steps: 15, steps per second: 143, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.075 [-1.806, 2.681], loss: 3.220839, mae: 5.768362, mean_q: 10.650168
 2510/5000: episode: 232, duration: 0.067s, episode steps: 8, steps per second: 119, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.537, 2.553], loss: 1.952215, mae: 5.509935, mean_q: 10.324883
 2523/5000: episode: 233, duration: 0.104s, episode steps: 13, steps per second: 125, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.084 [-1.800, 2.719], loss: 3.221525, mae: 5.765521, mean_q: 10.620461
 2532/5000: episode: 234, duration: 0.119s, episode steps: 9, steps per second: 76, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [-1.771, 2.890], loss: 2.951054, mae: 5.700300, mean_q: 10.422834
 2542/5000: episode: 235, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.148 [-1.741, 2.717], loss: 3.352693, mae: 5.684773, mean_q: 10.256456
 2551/5000: episode: 236, duration: 0.076s, episode steps: 9, steps per second: 119, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.145 [-1.615, 2.499], loss: 1.672804, mae: 5.582770, mean_q: 10.346442
 2560/5000: episode: 237, duration: 0.073s, episode steps: 9, steps per second: 124, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.162 [-1.539, 2.506], loss: 1.829656, mae: 5.728005, mean_q: 10.699245
 2568/5000: episode: 238, duration: 0.066s, episode steps: 8, steps per second: 121, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.129 [-1.415, 2.222], loss: 1.848089, mae: 5.487834, mean_q: 10.263300
 2577/5000: episode: 239, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.129 [-1.596, 2.430], loss: 1.406950, mae: 5.487480, mean_q: 10.349284
 2589/5000: episode: 240, duration: 0.111s, episode steps: 12, steps per second: 108, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.558, 2.351], loss: 1.442877, mae: 5.541183, mean_q: 10.545211
 2599/5000: episode: 241, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.765, 2.718], loss: 2.043098, mae: 5.511847, mean_q: 10.367666
 2609/5000: episode: 242, duration: 0.104s, episode steps: 10, steps per second: 97, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.110 [-1.610, 2.384], loss: 2.848136, mae: 5.627263, mean_q: 10.388089
 2619/5000: episode: 243, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.166 [-1.718, 2.747], loss: 2.138874, mae: 5.583807, mean_q: 10.401168
 2633/5000: episode: 244, duration: 0.118s, episode steps: 14, steps per second: 119, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.076 [-1.607, 2.565], loss: 2.370912, mae: 5.682052, mean_q: 10.514524
 2642/5000: episode: 245, duration: 0.085s, episode steps: 9, steps per second: 105, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.812, 2.805], loss: 1.893357, mae: 5.541641, mean_q: 10.394463
 2651/5000: episode: 246, duration: 0.078s, episode steps: 9, steps per second: 115, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.802, 2.860], loss: 2.213591, mae: 5.504269, mean_q: 10.158422
 2662/5000: episode: 247, duration: 0.090s, episode steps: 11, steps per second: 123, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.350, 2.222], loss: 1.864409, mae: 5.331415, mean_q: 9.918646
 2670/5000: episode: 248, duration: 0.067s, episode steps: 8, steps per second: 120, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.608, 2.523], loss: 1.880924, mae: 5.409796, mean_q: 10.120658
 2680/5000: episode: 249, duration: 0.097s, episode steps: 10, steps per second: 103, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.102 [-1.794, 2.649], loss: 2.633463, mae: 5.626444, mean_q: 10.403429
 2692/5000: episode: 250, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.117 [-1.544, 2.459], loss: 1.647643, mae: 5.421382, mean_q: 10.148308
 2700/5000: episode: 251, duration: 0.077s, episode steps: 8, steps per second: 105, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.557, 2.562], loss: 1.442813, mae: 5.507854, mean_q: 10.481379
 2710/5000: episode: 252, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.123 [-1.763, 2.684], loss: 2.029894, mae: 5.486893, mean_q: 10.344010
 2719/5000: episode: 253, duration: 0.082s, episode steps: 9, steps per second: 110, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.182 [-1.715, 2.869], loss: 3.091168, mae: 5.535779, mean_q: 10.157930
 2727/5000: episode: 254, duration: 0.064s, episode steps: 8, steps per second: 126, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.574, 2.565], loss: 1.335958, mae: 5.560977, mean_q: 10.561817
 2736/5000: episode: 255, duration: 0.073s, episode steps: 9, steps per second: 124, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.531, 2.453], loss: 1.725405, mae: 5.575242, mean_q: 10.448746
 2746/5000: episode: 256, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.535, 2.512], loss: 1.546775, mae: 5.489142, mean_q: 10.326099
 2754/5000: episode: 257, duration: 0.070s, episode steps: 8, steps per second: 114, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.532, 2.587], loss: 1.679109, mae: 5.482553, mean_q: 10.375505
 2763/5000: episode: 258, duration: 0.157s, episode steps: 9, steps per second: 57, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.743, 2.825], loss: 1.293857, mae: 5.480266, mean_q: 10.382470
 2772/5000: episode: 259, duration: 0.076s, episode steps: 9, steps per second: 119, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.770, 2.783], loss: 1.503981, mae: 5.336386, mean_q: 10.049158
 2783/5000: episode: 260, duration: 0.099s, episode steps: 11, steps per second: 111, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.127 [-1.582, 2.484], loss: 1.317330, mae: 5.219931, mean_q: 9.961834
 2791/5000: episode: 261, duration: 0.069s, episode steps: 8, steps per second: 116, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.583, 2.555], loss: 1.979645, mae: 5.428986, mean_q: 10.161282
 2802/5000: episode: 262, duration: 0.107s, episode steps: 11, steps per second: 103, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.103 [-1.401, 2.275], loss: 1.622866, mae: 5.416134, mean_q: 10.178498
 2810/5000: episode: 263, duration: 0.116s, episode steps: 8, steps per second: 69, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.131 [-1.377, 2.216], loss: 1.284927, mae: 5.484386, mean_q: 10.391520
 2822/5000: episode: 264, duration: 0.096s, episode steps: 12, steps per second: 125, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.101 [-1.388, 2.107], loss: 1.383366, mae: 5.504637, mean_q: 10.403531
 2831/5000: episode: 265, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.141 [-1.420, 2.180], loss: 1.253054, mae: 5.422533, mean_q: 10.265828
 2843/5000: episode: 266, duration: 0.146s, episode steps: 12, steps per second: 82, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-1.364, 2.090], loss: 1.341205, mae: 5.412982, mean_q: 10.245809
 2866/5000: episode: 267, duration: 0.236s, episode steps: 23, steps per second: 97, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.077 [-1.185, 1.699], loss: 1.940742, mae: 5.464263, mean_q: 10.173361
 2888/5000: episode: 268, duration: 0.196s, episode steps: 22, steps per second: 112, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.592, 0.967], loss: 1.691410, mae: 5.401613, mean_q: 10.053794
 2913/5000: episode: 269, duration: 0.210s, episode steps: 25, steps per second: 119, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.090 [-0.998, 1.462], loss: 1.213819, mae: 5.385776, mean_q: 10.123385
 2922/5000: episode: 270, duration: 0.091s, episode steps: 9, steps per second: 99, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.133 [-1.175, 1.866], loss: 1.451956, mae: 5.180333, mean_q: 9.642792
 2955/5000: episode: 271, duration: 0.229s, episode steps: 33, steps per second: 144, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.107 [-0.981, 1.480], loss: 1.646021, mae: 5.484690, mean_q: 10.207603
 2996/5000: episode: 272, duration: 0.340s, episode steps: 41, steps per second: 120, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.088 [-0.982, 2.111], loss: 1.204750, mae: 5.279868, mean_q: 9.894033
 3061/5000: episode: 273, duration: 0.503s, episode steps: 65, steps per second: 129, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.066 [-0.815, 1.116], loss: 1.256501, mae: 5.346012, mean_q: 10.007991
 3088/5000: episode: 274, duration: 0.243s, episode steps: 27, steps per second: 111, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.109 [-1.205, 0.389], loss: 1.376859, mae: 5.458794, mean_q: 10.250263
 3147/5000: episode: 275, duration: 0.414s, episode steps: 59, steps per second: 143, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.083 [-0.775, 1.702], loss: 1.098137, mae: 5.367993, mean_q: 10.083515
 3201/5000: episode: 276, duration: 0.492s, episode steps: 54, steps per second: 110, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.085 [-0.671, 0.356], loss: 0.948056, mae: 5.359171, mean_q: 10.110484
 3255/5000: episode: 277, duration: 0.439s, episode steps: 54, steps per second: 123, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.645, 1.375], loss: 1.220655, mae: 5.527686, mean_q: 10.410711
 3323/5000: episode: 278, duration: 0.457s, episode steps: 68, steps per second: 149, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.083 [-0.741, 0.657], loss: 1.112384, mae: 5.521977, mean_q: 10.402017
 3370/5000: episode: 279, duration: 0.407s, episode steps: 47, steps per second: 116, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.064 [-1.049, 0.347], loss: 1.065433, mae: 5.606731, mean_q: 10.579367
 3407/5000: episode: 280, duration: 0.347s, episode steps: 37, steps per second: 107, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.106 [-0.967, 0.605], loss: 1.016559, mae: 5.627885, mean_q: 10.629164
 3438/5000: episode: 281, duration: 0.285s, episode steps: 31, steps per second: 109, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.122 [-0.611, 0.437], loss: 0.967474, mae: 5.623010, mean_q: 10.625995
 3476/5000: episode: 282, duration: 0.297s, episode steps: 38, steps per second: 128, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.099 [-0.653, 0.376], loss: 1.547685, mae: 5.773953, mean_q: 10.796556
 3513/5000: episode: 283, duration: 0.279s, episode steps: 37, steps per second: 133, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.108 [-0.867, 0.537], loss: 1.002052, mae: 5.646477, mean_q: 10.600961
 3570/5000: episode: 284, duration: 0.447s, episode steps: 57, steps per second: 128, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.124 [-0.751, 0.361], loss: 1.034287, mae: 5.810724, mean_q: 10.968868
 3619/5000: episode: 285, duration: 0.351s, episode steps: 49, steps per second: 140, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.121 [-0.786, 0.676], loss: 1.126139, mae: 5.831433, mean_q: 11.005651
 3678/5000: episode: 286, duration: 0.418s, episode steps: 59, steps per second: 141, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.061 [-0.992, 0.168], loss: 1.319991, mae: 5.795579, mean_q: 10.882938
 3721/5000: episode: 287, duration: 0.318s, episode steps: 43, steps per second: 135, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.117 [-0.588, 0.353], loss: 1.312408, mae: 5.928089, mean_q: 11.127195
 3781/5000: episode: 288, duration: 0.460s, episode steps: 60, steps per second: 130, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.087 [-0.406, 0.816], loss: 1.201864, mae: 5.924433, mean_q: 11.171557
 3893/5000: episode: 289, duration: 0.821s, episode steps: 112, steps per second: 136, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.033 [-0.734, 0.796], loss: 1.330389, mae: 6.042617, mean_q: 11.343272
 3942/5000: episode: 290, duration: 0.326s, episode steps: 49, steps per second: 150, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.103 [-0.612, 0.443], loss: 1.598102, mae: 6.168009, mean_q: 11.554864
 3971/5000: episode: 291, duration: 0.251s, episode steps: 29, steps per second: 116, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.120 [-0.350, 0.820], loss: 1.182599, mae: 6.212670, mean_q: 11.735913
 4032/5000: episode: 292, duration: 0.433s, episode steps: 61, steps per second: 141, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.128 [-0.854, 0.408], loss: 1.344088, mae: 6.315542, mean_q: 11.936125
 4079/5000: episode: 293, duration: 0.382s, episode steps: 47, steps per second: 123, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.101 [-0.921, 0.619], loss: 1.660889, mae: 6.283567, mean_q: 11.791765
 4121/5000: episode: 294, duration: 0.301s, episode steps: 42, steps per second: 139, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.100 [-0.746, 0.374], loss: 1.897194, mae: 6.405807, mean_q: 11.956447
 4176/5000: episode: 295, duration: 0.391s, episode steps: 55, steps per second: 141, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.142 [-0.685, 0.478], loss: 1.387882, mae: 6.406605, mean_q: 12.072013
 4218/5000: episode: 296, duration: 0.294s, episode steps: 42, steps per second: 143, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.093 [-1.077, 0.236], loss: 1.423934, mae: 6.498845, mean_q: 12.269928
 4314/5000: episode: 297, duration: 0.732s, episode steps: 96, steps per second: 131, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.024 [-0.985, 0.385], loss: 1.732018, mae: 6.656652, mean_q: 12.495972
 4381/5000: episode: 298, duration: 0.417s, episode steps: 67, steps per second: 161, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.113 [-0.935, 0.435], loss: 1.422118, mae: 6.653068, mean_q: 12.556179
 4432/5000: episode: 299, duration: 0.397s, episode steps: 51, steps per second: 128, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.101 [-0.479, 0.907], loss: 1.439511, mae: 6.664454, mean_q: 12.559703
 4517/5000: episode: 300, duration: 0.537s, episode steps: 85, steps per second: 158, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.106 [-0.356, 1.186], loss: 1.768207, mae: 6.846800, mean_q: 12.854452
